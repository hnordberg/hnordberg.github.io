<!DOCTYPE html><!--KGccG2SBkPbpMbdJyROfK--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/chunks/de8cec9fccaece6c.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/626cf01e57e92f6c.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/a4d58bef501c3fa7.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/2e777329c8f64894.js"/><script src="/_next/static/chunks/242188bb71c223d6.js" async=""></script><script src="/_next/static/chunks/8296bf97416a5ebf.js" async=""></script><script src="/_next/static/chunks/8008d994f91f0fb6.js" async=""></script><script src="/_next/static/chunks/turbopack-c607b3f22603c289.js" async=""></script><script src="/_next/static/chunks/dc293e429f126484.js" async=""></script><script src="/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/_next/static/chunks/247eb132b7f7b574.js" async=""></script><script src="/_next/static/chunks/ddbba66711dfefbf.js" async=""></script><script src="/_next/static/chunks/20d7cae2716d2798.js" async=""></script><title>Henrik Nordberg, Principal Engineer</title><meta name="description" content="Henrik Nordberg&#x27;s project showcase"/><link rel="shortcut icon" href="/favicon.ico"/><link rel="icon" href="/favicon.ico"/><link rel="apple-touch-icon" href="/favicon.ico"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><header><nav id="main-nav" class=""><div class="nav-mobile-header"><button class="hamburger-menu" aria-label="Toggle menu" aria-expanded="false"><span></span> <span></span><span></span></button><span class="mobile-name">Henrik Nordberg</span></div><ul class=""><li><a class="" href="/">Home</a></li><li><a class="active" href="/projects">Projects</a></li><li><a class="" href="/technology">Technology</a></li><li><a class="" href="/leadership">Leadership</a></li><li><a class="" href="/publications">Publications</a></li><li><a class="" href="/contact">Contact</a></li></ul></nav><div class="ThemeSwitcher-module__I_Ny3W__themeSwitchContainer"><input type="checkbox" id="checkbox" class="ThemeSwitcher-module__I_Ny3W__checkbox" title="Toggle theme" aria-label="Toggle theme"/><label for="checkbox" class="ThemeSwitcher-module__I_Ny3W__label"></label></div></header><main class="page-with-contents"><script>(self.__next_s=self.__next_s||[]).push([0,{"children":"\n            window.MathJax = {\n              tex: {\n                inlineMath: [['$', '$'], ['\\\\(', '\\\\)']],\n                displayMath: [['$$', '$$'], ['\\\\[', '\\\\]']],\n                processEscapes: true,\n                processEnvironments: true,\n                tags: 'ams'\n              },\n              svg: {\n                fontCache: 'global'\n              }\n            };\n          ","id":"MathJax-config"}])</script><div class="Contents-module__8mGdoq__contents"><div class="Contents-module__8mGdoq__contentsTitle">Contents</div><ul class="Contents-module__8mGdoq__contentsList"><li><button class="Contents-module__8mGdoq__contentsLink">JGI Genome Portal</button></li><li><button class="Contents-module__8mGdoq__contentsLink">Elviz: Metagenome Visualization</button></li><li><button class="Contents-module__8mGdoq__contentsLink">BioPig: Hadoop-based Genomic Analysis Toolkit</button></li><li><button class="Contents-module__8mGdoq__contentsLink">CommerceRoute Data Integration and Workflow</button></li><li><button class="Contents-module__8mGdoq__contentsLink">Multi-dimensional Clustering Algorithm</button></li><li><button class="Contents-module__8mGdoq__contentsLink">Compressed Bitmap Index</button></li><li><button class="Contents-module__8mGdoq__contentsLink">Query Estimator for Petabyte Storage Systems</button></li><li><button class="Contents-module__8mGdoq__contentsLink">Cosmic Microwave Background (CMB) Spectrum Analysis</button></li><li><button class="Contents-module__8mGdoq__contentsLink">Isotope Explorer</button></li><li><button class="Contents-module__8mGdoq__contentsLink">BatMUD</button></li><li><button class="Contents-module__8mGdoq__contentsLink">Firefox extension: Copy That</button></li><li><button class="Contents-module__8mGdoq__contentsLink">Photo Stats: A Python EXIF scanner</button></li><li><button class="Contents-module__8mGdoq__contentsLink">WebScan: A product / price / availability scanner</button></li></ul></div><section class="card-grid"><div class="card" id="jgi-genome-portal"><div class="card-title">JGI Genome Portal</div><div class="card-title-subtitle">2008 - 2014 • JavaScript, AngularJS, Perl</div><div class="card-text">The Department of Energy (DOE) Joint Genome Institute (JGI) is a national user facility with massive-scale DNA sequencing and analysis capabilities dedicated to advancing genomics for bioenergy and environmental applications. Beyond generating tens of trillions of DNA bases annually, the Institute develops and maintains data management systems and specialized analytical capabilities to manage and interpret complex genomic data sets, and to enable an expanding community of users around the world to analyze these data in different contexts over the web.<div style="display:inline-block;width:100%" class="pt-4 pb-4"><img alt="Genome Portal Landing Page" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/genome-portal-tree-of-life.jpg"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Tree of Life showing sequenced organisms and samples</div></div>The JGI Genome Portal (<a href="https://genome.jgi.doe.gov">genome.jgi.doe.gov</a>) provides a unified access point to all JGI genomic databases and analytical tools. A user can find all DOE JGI sequencing projects and their status, search for and download assemblies and annotations of sequenced genomes, and interactively explore those genomes and compare them with other sequenced microbes, fungi, plants or metagenomes using specialized systems tailored to each particular class of organisms. As Team Lead, I was responsible for this portal, which provides access to data at multiple stages of genome analysis and annotation pipelines.<br/><br/>We published a paper on it in Nucleic Acids Research in 2012: <a href="https://doi.org/10.1093/nar/gkr947">The Genome Portal of the Department of Energy</a>.</div><div class="card-subtitle pt-4">Technology</div><div class="card-text">The Genome Portal is built using a combination of technologies. The genome browser (annotation and assembly viewer) is a large Perl program. When I took over, I implemented parallel rendering of the different tracks (see image below). I also added a number of long-requested features.<div style="display:inline-block;width:100%" class="pt-4 pb-4"><img alt="Genome Portal browser" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/genome-portal-browser.png"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Genome browser implemented in Perl</div></div></div></div><div class="card" id="elviz-metagenome-visualization"><div class="card-title">Elviz: Metagenome Visualization</div><div class="card-title-subtitle">2015 • AngularJS, JavaScript, WebGL</div><div class="card-text">Elviz (Environmental Laboratory Visualization) is an interactive web-based tool for the visual exploration of assembled metagenomes and their complex metadata.<div style="display:inline-block;width:100%" class="pt-4 pb-4"><img alt="Elviz: metagenome visualization" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/elviz.png"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Interactive visualization of metagenome assemblies</div></div>Elviz allows scientists to navigate metagenome assemblies across multiple dimensions and scales, plotting parameters such as GC content, relative abundance, phylogenetic affiliation and assembled contig length. Furthermore, Elviz enables interactive exploration using real-time plot navigation, search, filters, axis selection, and the ability to drill from a whole-community profile down to individual gene annotations.</div><div class="card-subtitle pt-4">Technology</div><div class="card-text">Elviz is written in AngularJS, JavaScript, and WebGL. Try it out at <a href="https://genome.jgi.doe.gov/viz">genome.jgi.doe.gov/viz</a>. We published a paper on it in BMC Bioinformatics in 2015: <a href="https://doi.org/10.1186/s12859-015-0566-4">Elviz – exploration of metagenome assemblies with an interactive visualization tool</a>.</div></div><div class="card" id="biopig-hadoop-based-genomic-analysis-toolkit"><div class="card-title">BioPig: Hadoop-based Genomic Analysis Toolkit</div><div class="card-title-subtitle">2013 • Java, Python, Pig Latin, Hadoop</div><div class="card-text">BioPig is a Hadoop-based analytic toolkit designed to scale large-scale sequence analysis to data volumes that overwhelm traditional tools. It sits on top of Hadoop MapReduce and the Pig data-flow language to provide a higher-level, more programmable framework for bioinformatics tasks, with emphasis on scalability, portability, and ease of use. We published a paper on it in Bioinformatics in 2013: <a href="https://doi.org/10.1093/bioinformatics/btt528">BioPig: a Hadoop-based analytic toolkit for large-scale sequence data</a>.</div><div class="card-subtitle pt-4">Origin</div><div class="card-text">In 2010, as I was working on the Genome Portal, I collaborated with Dr. Zhong Wang&#x27;s Genome Analysis research group at the Joint Genome Institute (JGI) to develop BioPig. Researchers at the JGI assemble genomes from raw sequence data using supercomputers at NERSC. BioPig was envisioned as a way to scale beyond the current methods. It was written by me and Karan Bhatia, using Java and Pig Latin. The source code is <a href="https://github.com/JGI-Bioinformatics/biopig">available on GitHub</a>.</div><div class="card-subtitle pt-4">Performance and Scalability</div><div class="card-text">Empirical results compare BioPig against serial and MPI implementations, using datasets from 100 Mb (Mega basepairs) up to 500 Gb. BioPig demonstrates near-linear scaling with data size on Hadoop clusters (e.g., Magellan/NERSC and AWS EC2), whereas traditional serial/MPI approaches hit memory limits or scale poorly in practice due to high-latency or bespoke parallelization requirements. While there is a certain amount of overhead thanks to Hadoop’s startup, for very large datasets the data-analysis time dominates startup costs.<div style="display:inline-block;width:100%" class="pt-4"><img alt="BioPig performance and scalability" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/biopig-graphs.png"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Performance comparison showing near-linear scaling with data size</div></div></div><div class="card-subtitle pt-4">Example</div><div class="card-text">Below is a simple example of a Pig script to count kmers. For more advanced examples, see the <a href="https://github.com/JGI-Bioinformatics/biopig/tree/master/examples">examples</a> directory on GitHub, or the paper.<pre>-- a simple example of pig script to count kmers<br/>1 register /.../biopig-core-0.3.0-job-pig.jar<br/>2 A = load &#x27;$input&#x27; using gov.jgi.meta.pig.storage.FastaStorage as (id: chararray, d: int, seq: bytearray, header: chararray);<br/>3 B = foreach A generate flatten(gov.jgi.meta.pig.eval.KmerGenerator(seq, 20)) as (kmer:bytearray);<br/>4 C = group B by kmer parallel $p;<br/>5 D = foreach C generate group, count(B);<br/>6 E = group D by $1 parallel $p;<br/>7 F = foreach E generate group, count(D);<br/>8 store F into &#x27;$output&#x27;;<br/></pre></div></div><div class="card" id="commerceroute-data-integration-and-workflow-solutions"><div class="card-title">CommerceRoute Data Integration and Workflow Solutions</div><div class="card-title-subtitle">1997 - 2003 • C++, Microsoft MFC, Java</div><div class="card-text">In 1997 I was the engineering founder of what became CommerceRoute. The first product was a workflow/Business Process Modeling (BPM) offering. It featured a client tool for defining the flow, a rules database, and a rules engine. You would define a workflow process, with tasks linked together by routes. It didn&#x27;t have any restrictions on how complicated the flow could be, allowing recursive flows and multiple instances of the same task. Right away people wanted to pull in data from external sources, and use them in the logic. We started adding connections to databases, LDAP, SAP, FTP, local files, and formats like XML, EDI, flat files, etc. With time people started to use the flows for data movement instead of just process automation. That&#x27;s how the next product was born.</div><div class="card-subtitle pt-4">Data Integration</div><div class="card-text">Building upon the powerful engine, we developed the CommerceRoute data integration solution, which handles transformations and transfers between sources and targets, and implemented the RosettaNet B2B framework. The complete solution was later sold as the Syncx integration appliance, reducing support costs.</div><div class="card-subtitle pt-4">CommerceRoute SaaS</div><div class="card-text">The launch of CommerceRoute Syncx was accompanied by a new SaaS offering: commerceroute.net All Syncx appliances talked to commerceroute.net and reported health and usage data. As needed, updates were pushed to the appliances. This included both OS level security updates and new features.<div style="display:inline-block;width:100%" class="pt-4"><img alt="CommerceRoute Web Data Interchange Login" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/commerceroute-web-data-interchange-login.png"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">CommerceRoute SaaS web interface</div></div></div><div class="card-subtitle pt-4">Acquisition</div><div class="card-text">In 2003 a new company was formed to acquire the product. The new company was called Jitterbit, and it acquired the technology from CommerceRoute. A few of the founders of CommerceRoute stayed on to start Jitterbit. Jitterbit is now a cloud-based data integration company recognized as a leader and visionary in the iPaaS field.</div><div class="card-subtitle pt-4">Technology and Architecture</div><div class="card-text">The engine was written in C++ using boost libraries. The workflow client was written in C++ using Microsoft&#x27;s MFC library. For a while Jitterbit open sourced the product, and it is <a href="https://sourceforge.net/p/jitterbit/code/HEAD/tree/">available</a> on SourceForge. After a while Jitterbit completely stopped using the workflow product and focused on the data integration product. But it wasn&#x27;t until 2009 that they pushed an update to remove the workflow logic from the code. So you can still find my original code in the repository by <a href="https://sourceforge.net/p/jitterbit/code/30994/tree/trunk/">going back through the history</a>. For example the <a href="https://sourceforge.net/p/jitterbit/code/39302/tree/trunk/integration/cpp/konga/ProcessEngine/Engine/RoutingEngineCore.cpp">RoutingEngineCore.cpp</a> file handles the routing logic for the workflow.</div><div class="card-subtitle pt-4">Scaling</div><div class="card-text">The workflow engine is multi-threaded and will detect the number of cores available and use them. It is also able to scale as a cluster of machines. In this mode, synchronization is handled via the database. The standard Syncx appliance configuration started with 2 multi-core machines and one database server. From there, customers would add capacity as needed by adding more machines. This was designed to be a simple process and was done via the web interface. For enterprises, they would add a second database server, which would run in a two-phase commit mode to ensure data consistency. In that mode, each query is executed in a way that the transaction succeeds only after both database servers have confirmed a successful commit. During one demo we did to a large enterprise customer, we used a workflow with complex logic with loops and data dependencies between tasks. In the middle of the demo, we pulled the power to the machine and waited for it to restart. When it came back online, the process continued and completed successfully. This was possible because the source of truth for the workflow logic and state was always the database.</div></div><div class="card" id="multi-dimensional-clustering"><div class="card-title">Multi-dimensional Clustering Algorithm</div><div class="card-title-subtitle">1997 - 1999 • C++, Java</div><div class="card-text">When I joined the Scientific Data Management R&amp;D group at Lawrence Berkeley Laboratory in 1997, the first project I worked on was a multi-dimensional clustering algorithm. As the high energy physics community was preparing for the Large Hadron Collider, they looked to our group to help manage the data. My task was to come up with an algorithm to find clusters of collision events. An &quot;event&quot; is when two particles collide in a collider. The data describes how many of each type of elementary particle were produced in the collision, along with data for each particle, such as momentum and energy.</div><div class="card-subtitle pt-4">The Curse of Dimensionality</div><div class="card-text">We were dealing with 100 - 150 columns or dimensions of data. Classical clustering algorithms like k-means or hierarchical clustering break down when you have that many dimensions. This is known as the curse of dimensionality. The events tend to cluster into groups of events that are similar. My task was to find them.</div><div class="card-subtitle pt-4">Algorithm</div><div class="card-text">I relied on the fact that the data is very sparse in higher dimensions. Since the number of events was large but known, I decided to use a hash table to store information about only the non-empty cells. The algorithm was:<ul class="list"><li>Read the data (one pass) and populate the cells</li><li>Sort the cells by the number of events</li><li>Grow a cluster around the largest cell. Find all neighbors of Manhattan distance 1. Stop growing when events in a cell are below a threshold, or when the gradient increases.</li><li>Then grow the cluster around the <i>next available</i> largest cell. </li><li>Repeat until all cells are in a cluster</li></ul></div><div class="card-subtitle pt-4">Technology</div><div class="card-text">The algorithm was implemented in C++ on Solaris. I used a hash table to store the data. In addition to the algorithm, we also developed a Java Applet that could be used to explore the clusters.<div style="display:inline-block;width:100%" class="pt-4 pb-4"><img alt="Multi-dimensional Clustering" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/cluster-explorer-applet.png"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Java applet for exploring multi-dimensional clusters</div></div>Selecting the bins was a task of its own. The Java Applet allowed you to explore what bins yielded the best clusters.</div></div><div class="card" id="compressed-bitmap-index"><div class="card-title">Compressed Bitmap Index</div><div class="card-title-subtitle">1997 - 1999 • C++</div><div class="card-text">To be able to serve high energy physics data from tape systems, we needed to be able to quickly estimate the size of a result set for a particular query before the data retrieval request was executed. This would allow the scientists to refine their queries before retrieving the data. To this end, I researched and implemented a specialized compressed bitmap (a.k.a. bit-sliced) index that is highly effective for range queries. The queries were logical joins of ranges on several columns. A key feature of this work is the ability to perform query execution directly without needing to decompress the index first. One idea was to run clustering on the data, and re-order the events based on clusters. In 1995 Gennady Antoshenkov developed the Byte-aligned Bitmap Code (BBC), a well-known scheme for bitmap compression. Our work is similar, but we added several novel features to make them more efficient for our use case.</div><div class="card-subtitle pt-4">Algorithm</div><div class="card-text">We assume that the objects are stored in a certain order in the index, and this order does not change. Thus, we first generate vertical partitions for each of the 100 properties. If all the properties are short integers, the size of all the partitions is 20 GBs (100 × 2 × 10⁸ bytes). The size doubles if all were real numbers. The vertical partitions are stored on disk. Now the bit-sliced index is designed to be a concise representation of these partitions, so that it is much smaller and can be stored in memory. Since the properties we deal with are numeric, we can partition each dimension into bins. For example, we can partition the &quot;energy&quot; dimension into 1–2 GeV, 2–3 GeV, and so on. We then assign to each bin a bit vector, where a &quot;0&quot; means that the value for that object does not fall in that bin, and &quot;1&quot; means it does.<div style="display:inline-block;width:100%" class="pt-4 pb-4"><img alt="Partitions" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/bit-slices.png"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Example of bit-sliced index partitions</div></div>The figure shows an example where Property 1 was partitioned into 7 bins, Property 2 into 5 bins, etc. Note that only a single &quot;1&quot; can exist for each row of each property, since the value only falls into a single bin.<br/><br/>We now compress the vertical bit-vectors using a modified version of run-length encoding. The advantage of this scheme is that boolean operations can be performed directly on the compressed vectors without decompression. This scheme is particularly effective for highly skewed data such as High Energy Physics data. Our version of run-length encoding does not encode short sequences into counts. Instead, it represents them as-is. A single bit in each word indicates whether it is a count or a pattern. Results show a compression factor of 10-100. The choice of bins and bin boundaries has a significant impact on compression. Assume 100 properties, each with 10 bins. This requires 10¹¹ bits before compression. With a compression factor of 100, the space required is 10⁹ bits, or about 125 MBs. This can be stored permanently in memory. Note that it is not necessary to keep all bit slices in memory. Only the most relevant slices for a query need to be retained.</div><div class="card-subtitle pt-4">Logical Operations on Bit-Slices</div><div class="card-text">The compression scheme described above permits logical operations on the compressed bit-slices (bitmap columns). This is an important feature of the compression algorithm used, since it makes it possible to do the operations in memory. These operations take as input two compressed slices and produce one compressed slice (the input for &quot;negation&quot; is only one bit-slice). All logical operations are implemented the same way:<ul class="list"><li>The state [&quot;0&quot; or &quot;1&quot;] at the current position and the number of bits of the current run (number of consecutive bits of that same state), &#x27;num&#x27;, from each bit-slice, are extracted (decoded).</li><li>The result is created (encoded) by performing the required logical operation (AND, OR, XOR) on the state bits from each bit-slice and subtracting the smaller &#x27;num&#x27; from the larger and appending the result to the resulting bit-slice. The resulting bit-slice is encoded as we go along, using the most efficient method for the size of its run lengths.</li></ul>An example: pseudo code for logical or:<pre>function bmp_or( bitslice left,
                 bitslice right )
{
  while( there_are_more_bits(left) 
    and there_are_more_bits(right) )
  {
    lbit = decode( left, lnum );
    rbit = decode( right, rnum );
    result_bit = lbit | rbit;
    result_num = min( lnum, rnum );

    lnum = lnum - result_num;
    rnum = rnum - result_num;
    encode( result, 
            result_bit,
            result_num );
  }
  return result;
}</pre></div></div><div class="card" id="cosmic-microwave-background-cmb-spectrum-analysis"><div class="card-title">Cosmic Microwave Background (CMB) Spectrum Analysis</div><div class="card-title-subtitle">1996 - 1998 • Fortran</div><div class="card-text">This research involved a detailed analysis of observations related to the Cosmic Microwave Background (CMB) radiation intensity, a relic of the Big Bang. In 1995 and 1996 I worked on the Isotope Explorer (see below) while also completing MSc in Physics, taking classes at UC Berkeley. When it came time to write my thesis, Dr. Richard Firestone of the Isotopes Project at LBL put me in touch with Dr. George Smoot, who had recently gathered data from his DMR experiment onboard the COBE satellite.</div><div class="card-subtitle pt-4">Nobel Prize</div><div class="card-text">The DMR data showed that the CMB was not uniform, but had a large difference in one direction, and small variations all around. This dipole pattern showed the motion of our solar system, and our local group of galaxies relative to the CMB. George received the Nobel Prize in Physics in 2006 for this work.<div style="display:inline-block;width:100%" class="pt-4 pb-4"><img alt="Dr. George Smoot and Dr. Steven Chu" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/george-smoot.jpg"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Dr. Chu and Dr. Smoot on the day of the Nobel Prize announcement. Photo: Henrik Nordberg</div></div>The image shows Dr. George Smoot and Dr. Steven Chu, who was the lab director at the time and also the Secretary of Energy, on the day of the announcement of the Nobel Prize in Physics in October 2006. Dr. Chu received the Nobel Prize in Physics in 1997. As a side note, Berkeley Lab is an amazing place. George would tell me stories about his advisor, Luis Alvarez, who also was a Nobel Laureate in Physics (1968). So you know, I still have a few years... :) I would attend the INPA (Institute for Nuclear and Particle Astrophysics) seminars on a regular basis. Whenever a new important paper in physics was published, the authors would come and give a talk. The lab has had 17 Nobel Prize winners affiliated with it.</div><div class="card-subtitle pt-4">Data Analysis</div><div class="card-text">In addition to the COBE data, George and I collected all known published data on the CMB spectrum. I then wrote Fortran code to fit the data to various models.<div style="display:inline-block;width:100%" class="pt-4 pb-4"><img alt="CMB spectrum analysis" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/cmb-spectrum.png"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Brightness as a function of frequency of the CMB</div></div><div style="display:inline-block;width:100%" class="pt-4"><img alt="Thermodynamic temperature as a function of frequency of the CMB dipole" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/cmb-dipole.png"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Thermodynamic temperature as a function of frequency of the CMB dipole</div></div></div><div class="card-subtitle pt-4">Levenberg-Marquardt Method</div><div class="card-text">I first tried gradient descent to fit the data to a model. It didn&#x27;t converge in a reasonable time.<div><p>
    The Levenberg-Marquardt method is used to iterate to the optimal $\chi^2$.
    It combines the Steepest Decent and Grid Search methods into an
    algorithm with a fast convergence.
    By Taylor expansion of $\chi^2$ we have
</p>

$$\begin{multline*}\chi^2 = \chi^2(0)+\sum_i \frac{\partial \chi^2}{\partial x_i}x_i\\+\frac{1}{2}\sum_{i,j}\frac{\partial^2\chi^2}{\partial x_i\partial x_j}x_ix_j+...\end{multline*}$$

<p>which we can write as</p>

$$\chi^2\approx \gamma - \vec{d}\cdot\vec{a}+{1\over 2}\vec{a}^T\cdot{\bf D}\cdot\vec{a} \tag{1}\label{eq:parabola}$$

<p>where $\vec{d}$ is a vector and ${\bf D}$ is a square matrix.
This leads to</p>

$$a_{\rm min}\approx a_{\rm cur}+{\bf D}^{-1}\cdot\left[-\nabla \chi^2(a_{\rm cur})\right] \tag{2}\label{eq:just_above}$$

<p>We measure $x_i$ and $s_i$. Our fitting function is $t(\vec{a})$.</p>

$$\chi ^2 = \sum_{q,r}{\left\{ (s-t(\vec{a}))_q
            {\bf M}_{qr}^{-1} (s-t(\vec{a}))_r \right\}}$$

$${\partial \chi ^2 \over \partial a_i} = -2\sum_{q,r}{\left\{
            \left({\partial t(\vec{a}) \over \partial a_i}\right)_q
            {\bf M}_{qr}^{-1} (s-t(\vec{a}))_r \right\}}$$

<div style="margin-left: -2em; font-size: 90%;">$$\begin{multline*}{\partial ^2 \chi ^2 \over \partial a_i \partial a_j} = 2\sum_{q,r}\left[
            \left\{\left({\partial t(\vec{a}) \over \partial a_i}\right)_q
            {\bf M}_{qr}^{-1} \left({\partial t(\vec{a}) \over \partial a_j}\right)_r \right\}-
            \right.\\ \left.
            \left\{\left({\partial^2 t(\vec{a}) \over \partial a_i\partial a_j}\right)_q
            {\bf M}_{qr}^{-1} (s-t(\vec{a}))_r \right\}
            \right]\end{multline*}$$</div>

<p>Define</p>

$$\beta_i\equiv -{1\over 2}{\partial \chi^2 \over \partial a_i}$$

<p>and</p>

$$\alpha_{ij}\equiv {1\over 2}{\partial^2\chi^2 \over \partial a_i\partial a_j}$$

<p>We approximate $\alpha$ with [Press, W.H., et al. Numerical Recipes]</p>

$$\alpha_{ij}\approx\sum_{q,r}\left\{
            \left({\partial t(\vec{a}) \over \partial a_i}\right)_q
            {\bf M}_{qr}^{-1} \left({\partial t(\vec{a}) \over \partial a_j}\right)_r
    \right\}$$

<p>
    Following equation (\ref{eq:parabola}) we have ${\bf\alpha}={1\over 2}
    {\bf D}$ and we can re-write eq.~(\ref{eq:just_above}) as
</p>

$$\sum_i \alpha_{ij}\delta a_i=\beta_j$$

<p>Let</p>

$$\delta a_i={1\over \lambda \alpha_{ii}}\beta_i,$$

<p>where $\lambda$ is a small positive constant and</p>

$$\begin{aligned}
    \alpha_{ii}' &\equiv \alpha_{ii}(1+\lambda)  \\
    \alpha_{ij}' &\equiv \alpha_{ij}\;\;\;\;\;(i\neq j)
\end{aligned}$$

$$\sum_i \alpha_{ij}'\delta a_i=\beta_j \tag{3}\label{eq:numrec14.4.14}$$

<p>The idea of the Levenberg-Marquardt method is summarized in the following procedure</p>

<blockquote>
    <ol>
        <li>Compute $\chi^2(\vec{a})$.</li>
        <li>$\lambda \leftarrow 0.001$.</li>
        <li>Solve the linear equations (\ref{eq:numrec14.4.14}) for $\delta\vec{a}$
        and evaluate $\chi^2(\vec{a}+\delta\vec{a})$.</li>
        <li>If $\chi^2(\vec{a}+\delta\vec{a})$ is smaller than a certain threshold,
        then stop.</li>
        <li>If $\chi^2(\vec{a}+\delta\vec{a})\geq\chi^2(\vec{a})$, <i>increase</i>
        $\lambda$ by a factor 10. Go to Step 3.</li>
        <li>If $\chi^2(\vec{a}+\delta\vec{a})<\chi^2(\vec{a})$, <i>decrease</i>
        $\lambda$ by a factor 10. Go to Step 3.</li>
    </ol>
</blockquote>

<p>We also need to calculate the standard deviations, $\delta a_i$,</p>

$${\bf C}={\bf{\alpha}}^{-1}$$

<p>and thus,</p>

$$\delta a_i = \sqrt{\Delta \chi _\nu ^2}\sqrt{C_{ii}},$$

<p>
    where $\Delta \chi _\nu ^2$ is a change in $\chi ^2$ per degree of
    freedom corresponding to a change in one of the parameters $a_i$.
</p>

<p>
    The Levenberg-Marquardt method was applied to the CMB datasets with great success.
    The method has several times more rapid convergence than the Steepest Descent
    and Grid Search methods. The latter two were tried first, but neither had reasonable
    convergence times, as they were implemented.
</p></div></div><div class="card-subtitle pt-4">Summary</div><div class="card-text">This work shows strong evidence for the hot Big Bang model. The monopole CMB spectrum is very close to a blackbody spectrum. There is a lot more math and physics than I can fit here. I wrote my <a href="/henrik-thesis-final.pdf">Master&#x27;s thesis</a> on this, and Dr. Smoot and I wrote a paper with a <a href="https://arxiv.org/pdf/astro-ph/9805123">more extensive analysis of the data</a>. Probably the most fun result is how we put an upper bound on the tachyon coupling constant, and an upper bound on the mass of a second species of photon.<br/><br/>Sadly George passed away in 2025.</div></div><div class="card" id="query-estimator-for-petabyte-storage-systems"><div class="card-title">Query Estimator for Petabyte Storage Systems</div><div class="card-title-subtitle">1998 - 1999 • C++</div><div class="card-text">Retrieving large subsets of data from tape-based datasets is very expensive both in terms of computer and system resources, as well as the total elapsed time for a query to be completed.  Therefore, it is quite useful to provide an estimate of the amount of data and the number of files that need to be retrieved for a potential query. Often, users start a query, get frustrated with the length of time to perform the analysis, and abort the query. An important optimization strategy is to prevent such non-productive activities by providing a quick estimate of the size of the returned data, and a time estimate of how long it will take to retrieve the data. I designed and developed such a &quot;query estimator&quot; by taking advantage of the compressed bitmap index used to evaluate a query.<div style="display:inline-block;width:100%" class="pt-4 pb-4"><img alt="Query Estimator" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/qe-using-index.png"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Query Estimator</div></div></div><div class="card-subtitle pt-4">Technology</div><div class="card-text"><div style="display:inline-block;width:100%" class="pt-4 pb-4"><img alt="Storage Manager" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/storage-manager.png"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Storage Manager</div></div>The query estimator was written in C++ and boost libraries, running on Solaris and Linux. It talked to other components in the distributed system via CORBA.</div></div><div class="card" id="isotope-explorer"><div class="card-title">Isotope Explorer</div><div class="card-title-subtitle">1995 • C++, Borland OWL, Microsoft MFC</div><div class="card-text">In 1995 I moved to Berkeley to work on a visualization tool for nuclei of isotopes at Lawrence Berkeley Laboratory. Originally called VuENSDF, it is a tool for exploring the nuclear data from the ENSDF database. Up till then, when you needed to access nuclear energy level data, you used the Table of Isotopes (ToI), which was a thick book. ToI was published by the Isotopes Project, a research group at the Nuclear Science Division of LBL. That group was headed by Nobel Laureate Glenn T. Seaborg, who still checked in on us from time to time. Dr. Seaborg was famous for the discovery of the elements plutonium, americium, curium, berkelium, californium, einsteinium, fermium, mendelevium, nobelium, and seaborgium.<div style="display:inline-block;width:100%" class="pt-4"><img alt="Isotope Explorer" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/isotope-explorer.png"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Isotope Explorer visualization tool</div></div></div><div class="card-subtitle pt-4">Technology</div><div class="card-text">The tool was written in C++ and Borland&#x27;s OWL library for Windows. One weekend I was reading the C code for the Unix Telnet server and decided to see if I could talk to it from Isotope Explorer. This led to us adding access to a large document set of references to the nuclear data. This was a web service before the term was invented.</div><div class="card-subtitle pt-4">Features</div><div class="card-text">Isotope Explorer can display level drawings, coincidences, tables, band plots, nuclear charts, chart data and literature references.</div></div><div class="card" id="batmud"><div class="card-title">BatMUD</div><div class="card-title-subtitle">1991 • LPC</div><div class="card-text">A MUD (Multi-User Dungeon) is an online role-playing game. In 1991 I was at the university and joined <a href="https://www.bat.org/">BatMUD</a> as a player. Back then you accessed the game via a telnet client. I wizzed (reached level 20 and beat Tiamat) and started extending the game as all wizards do. This is how I discovered my love of programming. I started working on the backend, adding a &#x27;feelings&#x27; system and the first global event (orch raids), among other things. The coding for LPC MUDs is in LPC, an object-oriented version of C. My player character is the Archwizard Plura.</div></div><div class="card" id="copy-that"><div class="card-title">Firefox extension: Copy That</div><div class="card-title-subtitle">2025 • JavaScript • <a href="https://github.com/hnordberg/copy-that">GitHub Repository</a></div><div class="card-text">Copy-That is a Firefox extension that allows you to copy text or HTML from an element on the page to the clipboard.<ul class="list"><li>Element Selection: Click the extension icon to activate, then hover over any element to see it highlighted</li><li>Plain Text Copy: Click an element to copy its innerText (plain text content)</li><div style="display:inline-block;width:100%" class="pt-4 pb-4"><img alt="Copy That" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/copy-that.png"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Blue outline shows what element is being selected</div></div><li>HTML Copy Mode: Hold Shift while clicking to copy the element&#x27;s innerHTML as rich HTML (with plain text fallback)</li><li>Visual Feedback:<ul class="list"><li>Red outline for normal mode</li><li>Blue outline for HTML mode (Shift held)</li><li>Green outline after successful copy</li></ul></li><li>Keyboard Support: Press Escape to cancel selection mode</li></ul></div></div><div class="card" id="photostats"><div class="card-title">Photo Stats: A Python EXIF scanner</div><div class="card-title-subtitle">2025 • Python • <a href="https://github.com/hnordberg/photo-stats">GitHub Repository</a></div><div class="card-text">A Python tool for scanning your photo collection and displaying statistics about your photography, such as your favorite lens, focal length, camera, and more. It uses SciPy and a KD-tree to quickly search geolocation data, and display the nearest cities for your photos. The cities data comes from <a href="https://github.com/dr5hn/countries-states-cities-database">Countries, States, and Cities database</a> on GitHub.<div style="display:inline-block;width:100%" class="pt-4 pb-4"><img alt="Photo Stats" title="Click to enlarge" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" style="color:transparent;cursor:pointer;transition:opacity 0.2s" src="/img/photostats-screenshot.png"/><div style="font-size:0.875rem;font-style:italic;margin-top:0.5rem;text-align:center;line-height:1.4" class="image-caption">Photo Stats</div></div></div></div><div class="card" id="webscan"><div class="card-title">WebScan: A product / price / availability scanner</div><div class="card-title-subtitle">2023 • JavaScript • <a href="https://github.com/hnordberg/webscan">GitHub Repository</a></div><div class="card-text">WebScan continuously monitors configured websites, tracks product listings, and alerts you when new products appear. Both email and desktop notifications are supported. It&#x27;s particularly useful for monitoring e-commerce sites for new inventory, price changes, or specific product availability. You specify the website and a CSS selector for the element to monitor.</div></div><div class="card hidden"><div class="card-title">.</div><div class="card-text">.</div><div class="card-subtitle pt-4">.</div><div class="card-text">.</div></div></section></main><!--$--><!--/$--><footer><p>© <!-- -->2025<!-- --> Henrik Nordberg</p></footer><script src="/_next/static/chunks/2e777329c8f64894.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[58195,[\"/_next/static/chunks/dc293e429f126484.js\"],\"default\"]\n3:I[62863,[\"/_next/static/chunks/dc293e429f126484.js\"],\"default\"]\n4:I[39756,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n5:I[37457,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n6:I[47257,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"ClientPageRoot\"]\n7:I[63936,[\"/_next/static/chunks/dc293e429f126484.js\",\"/_next/static/chunks/ddbba66711dfefbf.js\",\"/_next/static/chunks/20d7cae2716d2798.js\"],\"default\"]\na:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"OutletBoundary\"]\nb:\"$Sreact.suspense\"\nd:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"ViewportBoundary\"]\nf:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"MetadataBoundary\"]\n11:I[68027,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n:HL[\"/_next/static/chunks/de8cec9fccaece6c.css\",\"style\"]\n:HL[\"/_next/static/chunks/626cf01e57e92f6c.css\",\"style\"]\n:HL[\"/_next/static/chunks/a4d58bef501c3fa7.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"KGccG2SBkPbpMbdJyROfK\",\"c\":[\"\",\"projects\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"projects\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/de8cec9fccaece6c.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/dc293e429f126484.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"children\":[[\"$\",\"header\",null,{\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"$L3\",null,{}]]}],[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2025,\" Henrik Nordberg\"]}]}]]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L6\",null,{\"Component\":\"$7\",\"serverProvidedParams\":{\"searchParams\":{},\"params\":{},\"promises\":[\"$@8\",\"$@9\"]}}],[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/626cf01e57e92f6c.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/a4d58bef501c3fa7.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/ddbba66711dfefbf.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/20d7cae2716d2798.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$La\",null,{\"children\":[\"$\",\"$b\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@c\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$Ld\",null,{\"children\":\"$@e\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lf\",null,{\"children\":[\"$\",\"$b\",null,{\"name\":\"Next.Metadata\",\"children\":\"$@10\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"8:{}\n9:\"$0:f:0:1:1:children:1:children:0:props:children:0:props:serverProvidedParams:params\"\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"12:I[27201,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"IconMark\"]\n10:[[\"$\",\"title\",\"0\",{\"children\":\"Henrik Nordberg, Principal Engineer\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Henrik Nordberg's project showcase\"}],[\"$\",\"link\",\"2\",{\"rel\":\"shortcut icon\",\"href\":\"/favicon.ico\"}],[\"$\",\"link\",\"3\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\"}],[\"$\",\"link\",\"4\",{\"rel\":\"apple-touch-icon\",\"href\":\"/favicon.ico\"}],[\"$\",\"$L12\",\"5\",{}]]\nc:null\n"])</script></body></html>