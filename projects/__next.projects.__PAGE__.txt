1:"$Sreact.fragment"
2:I[33716,["/_next/static/chunks/a5494b77204598c0.js","/_next/static/chunks/ddbba66711dfefbf.js","/_next/static/chunks/55891c9ce3e41292.js"],"default"]
3:I[48349,["/_next/static/chunks/a5494b77204598c0.js","/_next/static/chunks/ddbba66711dfefbf.js","/_next/static/chunks/55891c9ce3e41292.js"],"default"]
15:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"OutletBoundary"]
16:"$Sreact.suspense"
:HL["/_next/static/chunks/3012c019bc39fca1.css","style"]
:HL["/_next/static/chunks/a4d58bef501c3fa7.css","style"]
0:{"buildId":"Xmx0FfuxbXQaLjSm6He1g","rsc":["$","$1","c",{"children":[["$","main",null,{"className":"page-with-contents","children":[["$","$L2",null,{"articles":[{"id":"jgi-genome-portal","title":"JGI Genome Portal"},{"id":"elviz-metagenome-visualization","title":"Elviz -- metagenome visualization"},{"id":"biopig-hadoop-based-genomic-analysis-toolkit","title":"BioPig: Hadoop-based Genomic Analysis Toolkit"},{"id":"jitterbit-data-integration-platform-ipaas","title":"Jitterbit Data Integration Platform (iPaaS)"},{"id":"commerceroute-data-integration-and-workflow-solutions","title":"CommerceRoute Data Integration and Workflow"},{"id":"multi-dimensional-clustering","title":"Multi-dimensional Clustering Algorithm"},{"id":"query-estimator-for-petabyte-storage-systems","title":"Query Estimator for Petabyte Storage Systems"},{"id":"compressed-bitmap-index","title":"Compressed Bitmap Index"},{"id":"cosmic-microwave-background-cmb-spectrum-analysis","title":"Cosmic Microwave Background (CMB) Spectrum Analysis"},{"id":"isotope-explorer","title":"Isotope Explorer"},{"id":"batmud","title":"BatMUD"}]}],["$","section",null,{"className":"card-grid","children":[["$","div",null,{"className":"card","id":"jgi-genome-portal","children":[["$","div",null,{"className":"card-title","children":"JGI Genome Portal"}],["$","div",null,{"className":"card-text","children":["The Department of Energy (DOE) Joint Genome Institute (JGI) is a national user facility with massive-scale DNA sequencing and analysis capabilities dedicated to advancing genomics for bioenergy and environmental applications. Beyond generating tens of trillions of DNA bases annually, the Institute develops and maintains data management systems and specialized analytical capabilities to manage and interpret complex genomic data sets, and to enable an expanding community of users around the world to analyze these data in different contexts over the web.",["$","$L3",null,{"src":"/img/genome-portal-tree-of-life.jpg","alt":"Genome Portal Landing Page","width":800,"height":600,"className":"pt-4 pb-4"}],"The JGI Genome Portal (",["$","a",null,{"href":"https://genome.jgi.doe.gov","children":"genome.jgi.doe.gov"}],") provides a unified access point to all JGI genomic databases and analytical tools. A user can find all DOE JGI sequencing projects and their status, search for and download assemblies and annotations of sequenced genomes, and interactively explore those genomes and compare them with other sequenced microbes, fungi, plants or metagenomes using specialized systems tailored to each particular class of organisms. As Team Lead, I was responsible for this portal, which provides access to data at multiple stages of genome analysis and annotation pipelines."]}],["$","div",null,{"className":"card-subtitle pt-4","children":"Technology"}],["$","div",null,{"className":"card-text","children":["The Genome Portal is built using a combination of technologies. The genome browser (annotation and assembly viewer) is a large Perl program. When I took over, I implemented parallel rendering of the different tracks (see image below). I also added a number of long requested features.",["$","$L3",null,{"src":"/img/genome-portal-browser.png","alt":"Genome Portal browser","width":800,"height":600,"className":"pt-4 pb-4"}]]}]]}],["$","div",null,{"className":"card","id":"elviz-metagenome-visualization","children":[["$","div",null,{"className":"card-title","children":"Elviz -- metagenome visualization"}],["$","div",null,{"className":"card-text","children":[["$","$L3",null,{"src":"/img/elviz.png","alt":"Elviz -- metagenome visualization","width":800,"height":600,"className":"pb-4"}],"Elviz (Environmental Laboratory Visualization) is an interactive web-based tool for the visual exploration of assembled metagenomes and their complex metadata. Elviz allows scientists to navigate metagenome assemblies across multiple dimensions and scales, plotting parameters such as GC content, relative abundance, phylogenetic affiliation and assembled contig length. Furthermore Elviz enables interactive exploration using real-time plot navigation, search, filters, axis selection, and the ability to drill from a whole-community profile down to individual gene annotations."]}],"$L4","$L5"]}],"$L6","$L7","$L8","$L9","$La","$Lb","$Lc","$Ld","$Le","$Lf"]}]]}],["$L10","$L11","$L12","$L13"],"$L14"]}],"loading":null,"isPartial":false}
4:["$","div",null,{"className":"card-subtitle pt-4","children":"Technology"}]
5:["$","div",null,{"className":"card-text","children":["Elviz is written in AngularJS, JavaScript, and WebGL. Try it out at ",["$","a",null,{"href":"https://genome.jgi.doe.gov/viz","children":"genome.jgi.doe.gov/viz"}],". We published a paper on it in BMC Bioinformatics in 2015: ",["$","a",null,{"href":"https://doi.org/10.1186/s12859-015-0566-4","children":"Elviz – exploration of metagenome assemblies with an interactive visualization tool"}],"."]}]
6:["$","div",null,{"className":"card","id":"biopig-hadoop-based-genomic-analysis-toolkit","children":[["$","div",null,{"className":"card-title","children":"BioPig: Hadoop-based Genomic Analysis Toolkit"}],["$","div",null,{"className":"card-text","children":["BioPig is a Hadoop-based analytic toolkit designed to scale large-scale sequence analysis to data volumes that overwhelm traditional tools. It sits on top of Hadoop MapReduce and the Pig data-flow language to provide a higher-level, more programmable framework for bioinformatics tasks, with emphasis on scalability, portability, and ease of use. We published a paper on it in Bioinformatics in 2013: ",["$","a",null,{"href":"https://doi.org/10.1093/bioinformatics/btt528","children":"BioPig: a Hadoop-based analytic toolkit for large-scale sequence data"}],"."]}],["$","div",null,{"className":"card-subtitle pt-4","children":"Origin"}],["$","div",null,{"className":"card-text","children":["In 2010, as I was working on the Genome Portal, I collaborated with Dr. Zhong Wang's Genome Analysis research group at the Joint Genome Institute (JGI) to develop BioPig. Researchers at the JGI assemble genomes from raw sequence data using supercomputers at NERSC. BioPig was envisioned as a way to scale beyond the current methods. It was written by me and Karan Bhatia, using Java and Pig Latin. The source code is ",["$","a",null,{"href":"https://github.com/JGI-Bioinformatics/biopig","children":"available on GitHub"}],"."]}],["$","div",null,{"className":"card-subtitle pt-4","children":"Performance and Scalability"}],["$","div",null,{"className":"card-text","children":["Empirical results compare BioPig against serial and MPI implementations, using datasets from 100 Mb (Mega basepairs) up to 500 Gb. BioPig demonstrates near-linear scaling with data size on Hadoop clusters (e.g., Magellan/NERSC and AWS EC2), whereas traditional serial/MPI approaches hit memory limits or scale poorly in practice due to high-latency or bespoke parallelization requirements. While there is a certain amount of overhead thanks to Hadoop’s startup, for very large datasets the data-analysis time dominates startup costs.",["$","$L3",null,{"src":"/img/biopig-graphs.png","alt":"BioPig performance and scalability","width":800,"height":600,"className":"pt-4 pb-4"}]]}],["$","div",null,{"className":"card-subtitle pt-4","children":"Example"}],["$","div",null,{"className":"card-text","children":["Below is a simple example of a Pig script to count kmers. For more advanced examples, see the ",["$","a",null,{"href":"https://github.com/JGI-Bioinformatics/biopig/tree/master/examples","children":"examples"}]," directory on GitHub, or the paper.",["$","pre",null,{"children":["-- a simple example of pig script to count kmers",["$","br",null,{}],"1 register /.../biopig-core-0.3.0-job-pig.jar",["$","br",null,{}],"2 A = load '$input' using gov.jgi.meta.pig.storage.FastaStorage as (id: chararray, d: int, seq: bytearray, header: chararray);",["$","br",null,{}],"3 B = foreach A generate flatten(gov.jgi.meta.pig.eval.KmerGenerator(seq, 20)) as (kmer:bytearray);",["$","br",null,{}],"4 C = group B by kmer parallel $p;",["$","br",null,{}],"5 D = foreach C generate group, count(B);",["$","br",null,{}],"6 E = group D by $1 parallel $p;",["$","br",null,{}],"7 F = foreach E generate group, count(D);",["$","br",null,{}],"8 store F into '$output';",["$","br",null,{}]]}]]}]]}]
7:["$","div",null,{"className":"card","id":"jitterbit-data-integration-platform-ipaas","children":[["$","div",null,{"className":"card-title","children":"Jitterbit Data Integration Platform (iPaaS)"}],["$","div",null,{"className":"card-text","children":"As a co-founder and Chief Software Architect, I developed Jitterbit into a cloud-based data integration company recognized as a leader and visionary in the iPaaS field. The solution expanded a prior product to include the ability to call and host web services, offering visual mapping for integration across structures like XML, databases, LDAP directories, and multiple cloud applications (e.g., Salesforce.com, NetSuite)."}]]}]
8:["$","div",null,{"className":"card","id":"commerceroute-data-integration-and-workflow-solutions","children":[["$","div",null,{"className":"card-title","children":"CommerceRoute Data Integration and Workflow Solutions"}],["$","div",null,{"className":"card-text","children":"This suite of products began with the architecture and implementation of SilkRoute (later WebWorkflow), a core workflow/Business Process Modeling product featuring a client tool, a rule database, and an engine written in C++ and ported to Linux and Solaris. Building upon this powerful engine, we developed the CommerceRoute data integration solution, which handles transformations and transfers between sources and targets (including FTP, SAP, databases, and formats like XML/EDI) and implements the RosettaNet B2B framework. The complete solution was later sold as the Syncx integration appliance, reducing support costs by limiting customer access strictly to a web browser."}]]}]
9:["$","div",null,{"className":"card","id":"query-estimator-for-petabyte-storage-systems","children":[["$","div",null,{"className":"card-title","children":"Query Estimator for Petabyte Storage Systems"}],["$","div",null,{"className":"card-text","children":"This component was developed as part of a Department of Energy (DOE) Grand Challenge project focused on handling petabytes (in 1998) of data stored on robotic tape systems (HPSS). The Query Estimator utilizes a compressed bitmap index I researched and implemented to quickly estimate the size of a result set for a particular query before the data retrieval request is executed, optimizing access to massive multi-dimensional datasets. The distributed system coordinating this storage access relied on CORBA for inter-component communication."}]]}]
a:["$","div",null,{"className":"card","id":"compressed-bitmap-index","children":[["$","div",null,{"className":"card-title","children":"Compressed Bitmap Index"}],["$","div",null,{"className":"card-text","children":"I researched and implemented a specialized compressed bitmap index that is highly effective for range queries. A key feature of this work is the ability to perform query execution directly without needing to decompress the index first, enhancing performance for high-dimensional data problems. This work was published in a scientific paper titled \"Notes on Design and Implementation of Compressed Bit Vectors\"."}]]}]
b:["$","div",null,{"className":"card","id":"multi-dimensional-clustering","children":[["$","div",null,{"className":"card-title","children":"Multi-dimensional Clustering Algorithm"}],["$","div",null,{"className":"card-text","children":"When I joined the Scientific Data Managment R&D group at Lawrence Berkeley Laboratory in 1997, the first project I worked on was a multi-dimensional clustering algorithm. As the high energy physics community was preparing for the Large Hadron Collider, they looked to our group to help manage the data. My task was to come up with an algorithm to find clusters of collision events. An \"event\" is when two particles collide in a collider. The data describes how many of each type of elementary particle were produced in the collision, along with data for each particle, such is momentum and energy. We were dealing with 100 - 150 columns or dimensions of data. The events tend to cluster into groups of events that are similar. My task was to find them."}],["$","div",null,{"className":"card-subtitle pt-4","children":"Algorithm"}],["$","div",null,{"className":"card-text","children":["Classical clustering algorithms like k-means or hierarchical clustering break down when you have that many dimensions. I relied on the fact that the data is very sparse in higher dimensions. Since the number of events was large but known, I decided to use a hash table to store information about only the non-empty cells. The algorithm was:",["$","ul",null,{"className":"list","children":[["$","li",null,{"children":"Read the data (one pass) and populate the cells"}],["$","li",null,{"children":"Sort the cells by the number of events"}],["$","li",null,{"children":"Grow cluster around the largest cell. Find all neighbors of Manhattan distance 1. Stop growing when events in a cell are below a threshold, or when the gradient increases."}],["$","li",null,{"children":["Then grow the cluster around the ",["$","i",null,{"children":"next available"}]," largest cell. "]}],["$","li",null,{"children":"Repeat until all cells are in a cluster"}]]}]]}],["$","div",null,{"className":"card-subtitle pt-4","children":"Technology"}],["$","div",null,{"className":"card-text","children":["The algorithm was implemented in C++ on Solaris. I used a hash table to store the data. In addition to the algorithm, we also developed a Java Applet that could be used to explore the clusters.",["$","$L3",null,{"src":"/img/cluster-explorer-applet.png","alt":"Multi-dimensional Clustering","width":800,"height":600,"className":"pt-4 pb-4"}],"Selecting the bins was a task of its own. The Java Applet allowed you to explorer what bins yielded the best clusters."]}]]}]
c:["$","div",null,{"className":"card","id":"cosmic-microwave-background-cmb-spectrum-analysis","children":[["$","div",null,{"className":"card-title","children":"Cosmic Microwave Background (CMB) Spectrum Analysis"}],["$","div",null,{"className":"card-text","children":"This research involved a detailed analysis of observations related to the Cosmic Microwave Background (CMB) radiation intensity, a relic of the Big Bang. The resulting analysis provided new, more precise values for the best fit temperature of the CMB (2.7356 ± 0.0038 K at 95% CL) and calculated the speed of our solar system relative to the CMB."}]]}]
d:["$","div",null,{"className":"card","id":"isotope-explorer","children":[["$","div",null,{"className":"card-title","children":"Isotope Explorer"}],["$","div",null,{"className":"card-text","children":["In 1995 I moved to Berkeley to work on a visualization tool for nuclei of isotopes at Lawrence Berkeley Laboratory. Originally called VuENSDF, it is a tool for exploring the nuclear data from the ENSDF database. Up till then, when you needed to access nuclear energy level data, you used the Table of Isotopes (ToI), which was a thick book. ToI was published by the Isotopes Project, a research group at the Nuclear Science Division of LBL. That group was headed by Nobel Laureate Glenn T. Seaborg, who still checked in on us from time to time. Dr. Seaborg was famous for the discovery of the elements plutonium, americium, curium, berkelium, californium, einsteinium, fermium, mendelevium, nobelium, and seaborgium.",["$","$L3",null,{"src":"/img/isotope-explorer.png","alt":"Isotope Explorer","width":800,"height":600,"className":"pt-4"}]]}],["$","div",null,{"className":"card-subtitle pt-4","children":"Technology"}],["$","div",null,{"className":"card-text","children":"The tool was written in C++ and Borland's OWL library for Windows. One weekend I was reading the C code for the Unix Telnet server and decided to see if I could talk to it from Isotope Explorer. This led to us adding access to a large document set of references to the nuclear data. This was a web service before the term was invented."}],["$","div",null,{"className":"card-subtitle pt-4","children":"Features"}],["$","div",null,{"className":"card-text","children":"Isotope Explorer can display level drawings, coincidences, tables, band plots, nuclear charts, chart data and literature references."}]]}]
e:["$","div",null,{"className":"card","id":"batmud","children":[["$","div",null,{"className":"card-title","children":"BatMUD"}],["$","div",null,{"className":"card-text","children":"A MUD (Multi-User Dungeon) is an online role-playing game. In 1991 I was at the university and joined BatMUD as a player. Back then you accessed the game via a telnet client. I wizzed (reached level 20 and beat Tiamat) and started extending the game as all wizards do. This is how I discovered my love of programming. I started working on the backend, adding a 'feelings' system and the first global event (orch raids), among other things. The coding for LPC MUDs is in LPC, an object-oriented version of C. My player character is the Archwizard Plura."}]]}]
f:["$","div",null,{"className":"card hidden","children":[["$","div",null,{"className":"card-title","children":"."}],["$","div",null,{"className":"card-text","children":"."}],["$","div",null,{"className":"card-subtitle pt-4","children":"."}],["$","div",null,{"className":"card-text","children":"."}]]}]
10:["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/3012c019bc39fca1.css","precedence":"next"}]
11:["$","link","1",{"rel":"stylesheet","href":"/_next/static/chunks/a4d58bef501c3fa7.css","precedence":"next"}]
12:["$","script","script-0",{"src":"/_next/static/chunks/ddbba66711dfefbf.js","async":true}]
13:["$","script","script-1",{"src":"/_next/static/chunks/55891c9ce3e41292.js","async":true}]
14:["$","$L15",null,{"children":["$","$16",null,{"name":"Next.MetadataOutlet","children":"$@17"}]}]
17:null
