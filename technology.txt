1:"$Sreact.fragment"
2:I[58195,["/_next/static/chunks/a5494b77204598c0.js"],"default"]
3:I[62863,["/_next/static/chunks/a5494b77204598c0.js"],"default"]
4:I[39756,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"default"]
5:I[37457,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"default"]
6:I[33716,["/_next/static/chunks/a5494b77204598c0.js","/_next/static/chunks/daa287a218aad508.js"],"default"]
13:I[68027,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"default"]
:HL["/_next/static/chunks/f9018588d3f1ee38.css","style"]
:HL["/_next/static/chunks/3012c019bc39fca1.css","style"]
0:{"P":null,"b":"TcCr40uwdoG0ceyMyyl4m","c":["","technology"],"q":"","i":false,"f":[[["",{"children":["technology",{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/f9018588d3f1ee38.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/_next/static/chunks/a5494b77204598c0.js","async":true,"nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"children":[["$","header",null,{"children":[["$","$L2",null,{}],["$","$L3",null,{}]]}],["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}],["$","footer",null,{"children":["$","p",null,{"children":["Â© ",2025," Henrik Nordberg"]}]}]]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[["$","main",null,{"className":"page-with-contents","children":[["$","$L6",null,{"articles":[{"id":"hard-problem-llms","title":"The hard problem of LLMs"}]}],["$","section",null,{"className":"card-grid","children":["$","div",null,{"className":"card","id":"hard-problem-llms","children":[["$","div",null,{"className":"card-text","children":["$","i",null,{"children":"Note: these are some musings on similarities LLMs and human thought"}]}],["$","div",null,{"className":"card-title","children":"The hard problem of LLMs"}],["$","div",null,{"className":"card-text","children":"The title is a play on the hard problem of consciousness and how LLMs may approach being conscious. The problem is stated something like how do the material processes of the brain give rise to human consciousness. This is one of my favorite problems in philosophy and now is the first time I have seen something of a possible answer to it. Let me explain."}],["$","div",null,{"className":"card-subtitle pt-4","children":"Embeddings and understanding"}],["$","div",null,{"className":"card-text","children":["The weights of an LLM can be seen as representing concepts. They are embeddings into a multi-dimensional space. People found that subtracting the embedding vector for woman from the one for queen gives us a vector close to the one we would get if we subtract v(man) from v(king) [",["$","a",null,{"href":"https://arxiv.org/pdf/1301.3781","children":"1"}],"]. This seems to indicate that there is space of concepts that we can index into. Calling this ",["$","i",null,{"children":"understand"}]," may be a bit premature, but let's continue."]}],["$","div",null,{"className":"card-subtitle pt-4","children":"Platonic representation"}],["$","div",null,{"className":"card-text","children":["You might wonder if the embeddings of one model are the same as another. No, they're probably not. But there does seem to be a relationship still. Perhaps the relationships of the vectors in one model's embedding space, are similar to the ones in a different model's embedding space? Yes, this seems more likely. The idea is called the Platonic Representation Hypothesis [","$L7","]."]}],"$L8","$L9","$La","$Lb","$Lc","$Ld","$Le"]}]}]]}],["$Lf","$L10"],"$L11"]}],{},null,false,false]},null,false,false]},null,false,false],"$L12",false]],"m":"$undefined","G":["$13",[]],"s":false,"S":true}
14:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"OutletBoundary"]
15:"$Sreact.suspense"
17:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"ViewportBoundary"]
19:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/247eb132b7f7b574.js"],"MetadataBoundary"]
7:["$","a",null,{"href":"https://arxiv.org/pdf/2405.07987","children":"2"}]
8:["$","div",null,{"className":"card-subtitle pt-4","children":"Are you thinking what I'm thinking?"}]
9:["$","div",null,{"className":"card-text","children":["Could we transform the embeddings from one model to another? Yes, it looks like we can. ",["$","a",null,{"href":"https://arxiv.org/pdf/2505.12540","children":"Jha et al. (2025)"}]," introduced the first method for translating text embeddings from one vector space to another without any paired data, encoders, or predefined matches. While this has security implications, it could also lead to innovations. What if we used the vector spaces from multiple models and try to derive a more complete space, or one with more accurate vectors (such that they better represent platonic semantic meanings)?"]}]
a:["$","div",null,{"className":"card-subtitle pt-4","children":"Is word generation thinking?"}]
b:["$","div",null,{"className":"card-text","children":["To answer that question we should define what thinking is. Google Gemini said it's \"Thinking is the mental process of manipulating information to form concepts, solve problems, make decisions, and understand the world.\"; Grok said \"cognitive process of using one's mind to consider, reason about, or manipulate ideas...\"; Others give similar definitions, but they seem a tad circular to me. \"mental process of manipulating ideas\" and \"using one's mind to consider\" are about the same to me, but don't get to the underlying mechanism. I like the idea of a ",["$","i",null,{"children":"train of thought"}]," better. Thinking is generating words in your mind, based on the words you thought last. The thing is there are multiple ways of thinking. The definition I just gave applies to when we are chatting with someone, or writing something like this paragraph. If someone asks us to do arithmetic, then we may be using a different part of the brain and the definition of thinking changes somewhat. But generating words (or concepts) based on previous words, is what LLMs do, so are they thinking? Yes, according to our definition, they are. But that doesn't mean they are conscious."]}]
c:["$","div",null,{"className":"card-subtitle pt-4","children":"Are we there yet?"}]
d:["$","div",null,{"className":"card-text","children":"If defining thinking is hard, defining consciousness is even harder. That said, LLMs are not conscious in the way we are, yet. They would need to have a continuous, though not infinite, context. They would need to selectively forget things, and be able to update their weights as they generate new information. But maybe we are more machine like than we thought."}]
e:["$","div",null,{"className":"card hidden","children":[["$","div",null,{"className":"card-title","children":"."}],["$","div",null,{"className":"card-text","children":"."}],["$","div",null,{"className":"card-subtitle pt-4","children":"."}]]}]
f:["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/3012c019bc39fca1.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]
10:["$","script","script-0",{"src":"/_next/static/chunks/daa287a218aad508.js","async":true,"nonce":"$undefined"}]
11:["$","$L14",null,{"children":["$","$15",null,{"name":"Next.MetadataOutlet","children":"$@16"}]}]
12:["$","$1","h",{"children":[null,["$","$L17",null,{"children":"$@18"}],["$","div",null,{"hidden":true,"children":["$","$L19",null,{"children":["$","$15",null,{"name":"Next.Metadata","children":"$@1a"}]}]}],null]}]
18:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
1a:[["$","title","0",{"children":"Henrik Nordberg, Principal Engineer"}],["$","meta","1",{"name":"description","content":"Henrik Nordberg's project showcase"}]]
16:null
