<!DOCTYPE html><!--MoUqpp0KonucF7RPubq1z--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/chunks/0f974d845c3e34b2.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/7948aa0b650af5c0.js"/><script src="/_next/static/chunks/72e2e806b02918c9.js" async=""></script><script src="/_next/static/chunks/557be5c833d730fe.js" async=""></script><script src="/_next/static/chunks/73a330e38f4c895c.js" async=""></script><script src="/_next/static/chunks/turbopack-e708d5680f2961b9.js" async=""></script><script src="/_next/static/chunks/c14175b176370ec3.js" async=""></script><script src="/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/_next/static/chunks/247eb132b7f7b574.js" async=""></script><title>Henrik Nordberg, Principal Engineer</title><meta name="description" content="Henrik Nordberg&#x27;s project showcase"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><header><nav id="main-nav"><ul><li><a class="" href="/">Home</a></li><li><a class="" href="/projects">Projects</a></li><li><a class="active" href="/technology">Technology</a></li><li><a class="" href="/leadership">Leadership</a></li><li><a class="" href="/publications">Publications</a></li><li><a class="" href="/contact">Contact</a></li></ul></nav><div class="ThemeSwitcher-module__I_Ny3W__themeSwitchContainer"><input type="checkbox" id="checkbox" class="ThemeSwitcher-module__I_Ny3W__checkbox" title="Toggle theme" aria-label="Toggle theme"/><label for="checkbox" class="ThemeSwitcher-module__I_Ny3W__label"></label></div></header><main><section class="card-grid"><div class="card"><div class="card-title">The hard problem of LLMs</div><div class="card-text">The title is a play on the hard problem of consciousness and how LLMs may approach being conscious. The problem is stated something like how do the material processes of the brain give rise to human consciousness. This is one of my favorite problems in philosophy and now is the first time I have seen something of a possible answer to it. Let me explain.</div><div class="card-subtitle pt-4">Embeddings and understanding</div><div class="card-text">The weights of an LLM can be seen as representing concepts. They are embeddings into a multi-dimensional space. People found that subtracting the embedding vector for woman from the one for queen gives us a vector close to the one we would get if we subtract v(man) from v(king) [<a href="https://arxiv.org/pdf/1301.3781">1</a>]. This seems to indicate that there is space of concepts that we can index into. Calling this <i>understand</i> may be a bit premature, but let&#x27;s continue.</div><div class="card-subtitle pt-4">Platonic representation</div><div class="card-text">You might wonder if the embeddings of one model are the same as another. No, they&#x27;re probably not. But there does seem to be a relationship still. Perhaps the relationships of the vectors in one model&#x27;s embedding space, are similar to the ones in a different model&#x27;s embedding space? Yes, this seems more likely. The idea is called the Platonic Representation Hypothesis [<a href="https://arxiv.org/pdf/2405.07987">2</a>].</div><div class="card-subtitle pt-4">Are you thinking what I&#x27;m thinking?</div><div class="card-text">Could we transform the embeddings from one model to another? Yes, it looks like we can. <a href="https://arxiv.org/pdf/2505.12540">Jha et al. (2025)</a> introduced the first method for translating text embeddings from one vector space to another without any paired data, encoders, or predefined matches. While this has security implications, it could also lead to innovations. What if we used the vector spaces from multiple models and try to derive a more complete space, or one with more accurate vectors (such that they better represent platonic semantic meanings)?</div><div class="card-subtitle pt-4">Is word generation thinking?</div><div class="card-text">To answer that question we should define what thinking is. Google Gemini said it&#x27;s &quot;Thinking is the mental process of manipulating information to form concepts, solve problems, make decisions, and understand the world.&quot;; Grok said &quot;cognitive process of using one&#x27;s mind to consider, reason about, or manipulate ideas...&quot;; Others give similar definitions, but they seem a tad circular to me. &quot;mental process of manipulating ideas&quot; and &quot;using one&#x27;s mind to consider&quot; are about the same to me, but don&#x27;t get to the underlying mechanism. I like the idea of a <i>train of thought</i> better. Thinking is generating words in your mind, based on the words you thought last. The thing is there are multiple ways of thinking. The definition I just gave applies to when we are chatting with someone, or writing something like this paragraph. If someone asks us to do arithmetic, then we may be using a different part of the brain and the definition of thinking changes somewhat. But generating words (or concepts) based on previous words, is what LLMs do, so are they thinking? Yes, according to our definition, they are. But that doesn&#x27;t mean they are conscious.</div><div class="card-subtitle pt-4">Are we there yet?</div><div class="card-text">If defining thinking is hard, defining consciousness is even harder. That said, LLMs are not conscious in the way we are, yet. They would need to have a continuous, though not infinite, context. They would need to selectively forget things, and be able to update their weights as they generate new information. But maybe we are more machine like than we thought.</div><div class="card hidden"><div class="card-title">.</div><div class="card-text">.</div><div class="card-subtitle pt-4">.</div></div></div></section></main><!--$--><!--/$--><footer><p>© <!-- -->2025<!-- --> Henrik Nordberg</p></footer><script src="/_next/static/chunks/7948aa0b650af5c0.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[58195,[\"/_next/static/chunks/c14175b176370ec3.js\"],\"default\"]\n3:I[62863,[\"/_next/static/chunks/c14175b176370ec3.js\"],\"default\"]\n4:I[39756,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n5:I[37457,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\ne:I[68027,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n:HL[\"/_next/static/chunks/0f974d845c3e34b2.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"MoUqpp0KonucF7RPubq1z\",\"c\":[\"\",\"technology\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"technology\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/0f974d845c3e34b2.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/c14175b176370ec3.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"children\":[[\"$\",\"header\",null,{\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"$L3\",null,{}]]}],[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2025,\" Henrik Nordberg\"]}]}]]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"main\",null,{\"children\":[\"$\",\"section\",null,{\"className\":\"card-grid\",\"children\":[\"$\",\"div\",null,{\"className\":\"card\",\"children\":[[\"$\",\"div\",null,{\"className\":\"card-title\",\"children\":\"The hard problem of LLMs\"}],[\"$\",\"div\",null,{\"className\":\"card-text\",\"children\":\"The title is a play on the hard problem of consciousness and how LLMs may approach being conscious. The problem is stated something like how do the material processes of the brain give rise to human consciousness. This is one of my favorite problems in philosophy and now is the first time I have seen something of a possible answer to it. Let me explain.\"}],[\"$\",\"div\",null,{\"className\":\"card-subtitle pt-4\",\"children\":\"Embeddings and understanding\"}],[\"$\",\"div\",null,{\"className\":\"card-text\",\"children\":[\"The weights of an LLM can be seen as representing concepts. They are embeddings into a multi-dimensional space. People found that subtracting the embedding vector for woman from the one for queen gives us a vector close to the one we would get if we subtract v(man) from v(king) [\",[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/1301.3781\",\"children\":\"1\"}],\"]. This seems to indicate that there is space of concepts that we can index into. Calling this \",[\"$\",\"i\",null,{\"children\":\"understand\"}],\" may be a bit premature, but let's continue.\"]}],[\"$\",\"div\",null,{\"className\":\"card-subtitle pt-4\",\"children\":\"Platonic representation\"}],[\"$\",\"div\",null,{\"className\":\"card-text\",\"children\":[\"You might wonder if the embeddings of one model are the same as another. No, they're probably not. But there does seem to be a relationship still. Perhaps the relationships of the vectors in one model's embedding space, are similar to the ones in a different model's embedding space? Yes, this seems more likely. The idea is called the Platonic Representation Hypothesis [\",[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2405.07987\",\"children\":\"2\"}],\"].\"]}],[\"$\",\"div\",null,{\"className\":\"card-subtitle pt-4\",\"children\":\"Are you thinking what I'm thinking?\"}],\"$L6\",\"$L7\",\"$L8\",\"$L9\",\"$La\",\"$Lb\"]}]}]}],null,\"$Lc\"]}],{},null,false,false]},null,false,false]},null,false,false],\"$Ld\",false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"f:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"OutletBoundary\"]\n10:\"$Sreact.suspense\"\n12:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"ViewportBoundary\"]\n14:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"MetadataBoundary\"]\n6:[\"$\",\"div\",null,{\"className\":\"card-text\",\"children\":[\"Could we transform the embeddings from one model to another? Yes, it looks like we can. \",[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2505.12540\",\"children\":\"Jha et al. (2025)\"}],\" introduced the first method for translating text embeddings from one vector space to another without any paired data, encoders, or predefined matches. While this has security implications, it could also lead to innovations. What if we used the vector spaces from multiple models and try to derive a more complete space, or one with more accurate vectors (such that they better represent platonic semantic meanings)?\"]}]\n7:[\"$\",\"div\",null,{\"className\":\"card-subtitle pt-4\",\"children\":\"Is word generation thinking?\"}]\n8:[\"$\",\"div\",null,{\"className\":\"card-text\",\"children\":[\"To answer that question we should define what thinking is. Google Gemini said it's \\\"Thinking is the mental process of manipulating information to form concepts, solve problems, make decisions, and understand the world.\\\"; Grok said \\\"cognitive process of using one's mind to consider, reason about, or manipulate ideas...\\\"; Others give similar definitions, but they seem a tad circular to me. \\\"mental process of manipulating ideas\\\" and \\\"using one's mind to consider\\\" are about the same to me, but don't get to the underlying mechanism. I like the idea of a \",[\"$\",\"i\",null,{\"children\":\"train of thought\"}],\" better. Thinking is generating words in your mind, based on the words you thought last. The thing is there are multiple ways of thinking. The definition I just gave applies to when we are chatting with someone, or writing something like this paragraph. If someone asks us to do arithmetic, then we may be using a different part of the brain and the definition of thinking changes somewhat. But generating words (or concepts) based on previous words, is what LLMs do, so are they thinking? Yes, according to our definition, they are. But that doesn't mean they are conscious.\"]}]\n9:[\"$\",\"div\",null,{\"className\":\"card-subtitle pt-4\",\"children\":\"Are we there yet?\"}]\na:[\"$\",\"div\",null,{\"className\":\"card-text\",\"children\":\"If defining thinking is hard, defining consciousness is even harder. That said, LLMs are not conscious in the way we are, yet. They would need to have a continuous, though not infinite, context. They would need to selectively forget things, and be able to update their weights as they generate new information. But maybe we are more machine like than we thought.\"}]\nb:[\"$\",\"div\",null,{\"className\":\"card hidden\",\"children\":[[\"$\",\"div\",null,{\"className\":\"card-title\",\"children\":\".\"}],[\"$\",\"div\",null,{\"className\":\"card-text\",\"children\":\".\"}],[\"$\",\"div\",null,{\"className\":\"card-subtitle pt-4\",\"children\":\".\"}]]}]\nc:[\"$\",\"$Lf\",null,{\"children\":[\"$\",\"$10\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@11\"}]}]\nd:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L12\",null,{\"children\":\"$@13\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L14\",null,{\"children\":[\"$\",\"$10\",null,{\"name\":\"Next.Metadata\",\"children\":\"$@15\"}]}]}],null]}]\n"])</script><script>self.__next_f.push([1,"13:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"15:[[\"$\",\"title\",\"0\",{\"children\":\"Henrik Nordberg, Principal Engineer\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Henrik Nordberg's project showcase\"}]]\n11:null\n"])</script></body></html>