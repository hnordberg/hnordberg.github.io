[
  {
    "id": "Perceptron",
    "period": "1957",
    "title": "Perceptron",
    "org": "Cornell Aeronautical Laboratory",
    "location": "https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf",
    "paperTitle": "The Perceptron: A Perceiving and Recognizing Automaton",
    "authors": [
      "Frank Rosenblatt"
    ],
    "description": "The Perceptron was the first model that could learn the weights defining categories given examples from each category. It established the foundation for artificial neural networks by introducing a learning algorithm that could automatically adjust connection weights. The perceptron demonstrated that machines could learn from experience, marking a fundamental breakthrough in machine learning.",
    "citations": 2525,
    "icon": "ml/cornell.svg",
    "details": "To reproduce Rosenblatt’s perceptron, start with a set of linearly separable training examples (e.g., 2D points labeled positive or negative). Initialize a weight vector and bias to zero (or small random values). For each training sample x with label y ∈ {−1, +1}, compute the activation a = w·x + b. If y·a ≤ 0 (the sample is misclassified), update the parameters with the perceptron rule: w ← w + η·y·x and b ← b + η·y, where η is a small learning rate (often 1.0 for the classic algorithm). Sweep through the dataset repeatedly until all samples are correctly classified or a maximum number of passes is reached. In code, this translates to a tight loop over the dataset with a dot product, a sign check, and in-place vector additions. To deploy the model, classify new inputs by computing w·x + b and returning the sign. Extensions include using homogeneous coordinates to fold the bias into the weight vector, shuffling examples between epochs to avoid cyclic updates, and visualizing the decision boundary w to confirm convergence. Although simple, this implementation demonstrates online learning, margin intuition, and the foundations of modern gradient-based training."
  },
  {
    "id": "Neocognitron",
    "period": "1980",
    "title": "Neocognitron",
    "org": "NHK Science & Technical Research Laboratories",
    "location": "https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf",
    "paperTitle": "Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position",
    "authors": [
      "Kunihiko Fukushima"
    ],
    "description": "The Neocognitron was a hierarchical, multilayered neural network inspired by the visual cortex. It introduced the concepts of S-cells (simple cells) and C-cells (complex cells) arranged in a hierarchy, allowing for position-invariant pattern recognition. This architecture laid the groundwork for modern convolutional neural networks and demonstrated that local feature extraction combined with spatial pooling could achieve robust visual recognition.",
    "citations": 6200,
    "icon": "ml/nhk.svg",
    "details": "To prototype a Neocognitron, stack multiple stages made of S-layers (convolutions with learned kernels plus nonlinear activation) followed by C-layers (local pooling/averaging with competition). Begin with small receptive fields (e.g., 5×5 filters) and progressively increase the number of feature maps per stage. During training, expose the network to input patterns translated across the visual field; learning adjusts S-layer kernels via unsupervised Hebbian/competitive rules, while C-layers perform max or average pooling to promote shift invariance. A modern replication can approximate the original by building a CNN that alternates conv → ReLU → local pooling, but to stay faithful include winner-take-all inhibition on S-cells and normalize responses within C-cells. Evaluate by feeding handwritten digits or symbols at various positions and verifying that final feature maps remain stable under shifts."
  },
  {
    "id": "Backpropagation",
    "period": "1986",
    "title": "Backpropagation",
    "org": "University of California San Diego, Carnegie Mellon University, University of Toronto",
    "location": "https://www.nature.com/articles/323533a0",
    "paperTitle": "Learning Representations by Back-propagating Errors",
    "authors": [
      "David E. Rumelhart",
      "Geoffrey E. Hinton",
      "Ronald J. Williams"
    ],
    "description": "Backpropagation provided an efficient method for training multi-layer neural networks by computing gradients through the chain rule. This algorithm enabled the training of deep networks by propagating error signals backwards through layers, allowing hidden units to learn internal representations. Backpropagation became the workhorse of neural network training and remains fundamental to modern deep learning.",
    "citations": 29607,
    "icon": "ml/ucsd.svg",
    "details": "Implement backprop by defining a feedforward network with differentiable layers (affine + activation). During the forward pass compute activations for each layer and the loss L(ŷ, y). For the backward pass, initialize δ at the output layer as ∂L/∂ŷ · σ′(z). Propagate gradients layer-by-layer using δᵗ = (Wᵗ⁺¹)ᵗ δ⁺¹ ⊙ σ′(zᵗ), and accumulate weight gradients ΔWᵗ = δᵗ (aᵗ⁻¹)ᵗ, Δbᵗ = δᵗ. Update parameters with learning rate η: Wᵗ ← Wᵗ − η·ΔWᵗ, bᵗ ← bᵗ − η·Δbᵗ. To stabilize training, use mini-batches, momentum, or adaptive optimizers. A simple reference implementation fits an MLP (784-128-64-10) on MNIST, using ReLU hidden units and softmax-cross-entropy output. Backprop generalizes to any computation graph: store intermediate values during forward pass, define local derivatives, and traverse the graph in reverse topological order to accumulate gradients."
  },
  {
    "id": "LSTM",
    "period": "1997",
    "title": "Long Short-Term Memory (LSTM)",
    "org": "Technische Universität München",
    "location": "https://deeplearning.cs.cmu.edu/S23/document/readings/LSTM.pdf",
    "paperTitle": "Long Short-Term Memory",
    "authors": [
      "Sepp Hochreiter",
      "Jürgen Schmidhuber"
    ],
    "description": "LSTM addressed the vanishing gradient problem in recurrent neural networks by introducing memory cells with gating mechanisms. The architecture uses input, output, and forget gates to control information flow, enabling networks to learn long-term dependencies. LSTM became the dominant architecture for sequence modeling tasks including speech recognition, machine translation, and time series prediction before the transformer era.",
    "citations": 139920,
    "icon": "ml/tum.gif",
    "details": "To code an LSTM layer, maintain cell state cₜ and hidden state hₜ. For each timestep xₜ, compute gates: iₜ = σ(Wᵢxₜ + Uᵢhₜ₋₁ + bᵢ), fₜ = σ(W_fxₜ + U_fhₜ₋₁ + b_f), oₜ = σ(Wₒxₜ + Uₒhₜ₋₁ + bₒ), ĝₜ = tanh(W_gxₜ + U_ghₜ₋₁ + b_g). Update cell state cₜ = fₜ ⊙ cₜ₋₁ + iₜ ⊙ ĝₜ and hidden state hₜ = oₜ ⊙ tanh(cₜ). Initialize c₀, h₀ to zeros. Backpropagation through time applies chain rule across timesteps, so cache gate activations and cell states to compute gradients efficiently. Practical tips: clip gradients to prevent explosion, use dropout on recurrent connections (e.g., variational dropout), and batch sequences with padding + masking. A minimal example trains an LSTM language model: embed tokens, pass through LSTM, project to vocabulary logits, compute cross-entropy, and optimize with Adam."
  },
  {
    "id": "LeNet",
    "period": "1998",
    "title": "Convolutional Neural Networks (LeNet)",
    "org": "AT&T Bell Laboratories",
    "location": "http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf",
    "paperTitle": "Gradient-Based Learning Applied to Document Recognition",
    "authors": [
      "Yann LeCun",
      "Léon Bottou",
      "Yoshua Bengio",
      "Patrick Haffner"
    ],
    "description": "LeNet introduced a practical convolutional neural network architecture for document recognition. It combined convolutional layers for local feature extraction, pooling for spatial invariance, and fully connected layers for classification. This architecture demonstrated that CNNs could be trained end-to-end using backpropagation and achieved state-of-the-art results on handwritten digit recognition, establishing the blueprint for modern computer vision systems.",
    "citations": 72400,
    "icon": "ml/bell_labs.svg",
    "details": "Recreate LeNet-5 with the original topology: input 32×32 grayscale image, conv1 (6 filters, 5×5, tanh) → subsampling (avg pooling, stride 2) → conv2 (16 filters, 5×5) → subsampling → conv3 (120 filters, 5×5) → flatten → FC1 (84 units) → FC2 (10 units, softmax). Use zero padding to preserve spatial dimensions in early layers. Train on centered MNIST digits, optionally augment with elastic distortions. Optimization: stochastic gradient descent with learning rate ≈0.01, momentum 0.9, and LeCun’s original weight normalization (fan-in scaling). Implement average pooling as learned subsampling (multiply by trainable weights and bias) if you want historical fidelity; otherwise max pooling works as a simplification. Evaluate by measuring test accuracy and visualizing feature maps to confirm hierarchical feature extraction."
  },
  {
    "id": "Neural_Probabilistic_LM",
    "period": "2003",
    "title": "The Neural Probabilistic Language Model",
    "org": "Université de Montréal",
    "location": "https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf",
    "paperTitle": "A Neural Probabilistic Language Model",
    "authors": [
      "Yoshua Bengio",
      "Réjean Ducharme",
      "Pascal Vincent",
      "Christian Jauvin"
    ],
    "description": "The Neural Probabilistic Language Model addressed the curse of dimensionality in language modeling by learning distributed representations for words. It introduced the idea that similar words would have similar vector representations, allowing the model to generalize to unseen word sequences. This foundational work pioneered the use of neural networks for language modeling and word embeddings, directly inspiring Word2Vec and modern language models.",
    "citations": 18500,
    "icon": "ml/montreal.png",
    "details": "Implement Bengio et al.’s model by (1) building a vocabulary V and mapping each word to a D-dimensional embedding vector; (2) concatenating embeddings for the previous n words to form context vector h; (3) passing h through a hidden layer with tanh and projecting to a |V|-dimensional softmax to produce next-word probabilities. Training minimizes negative log-likelihood using SGD. To improve efficiency, precompute and cache embeddings, use weight sharing for input/output matrices, and apply techniques like importance sampling or noise-contrastive estimation when V is large. Evaluation involves perplexity on held-out text: lower perplexity indicates better modeling of unseen sequences. This structure lays the groundwork for modern embedding layers and feedforward language models."
  },
  {
    "id": "ImageNet",
    "period": "2009",
    "title": "ImageNet Dataset",
    "org": "Princeton University",
    "location": "https://www.image-net.org/static_files/papers/imagenet_cvpr09.pdf",
    "paperTitle": "ImageNet: A Large-Scale Hierarchical Image Database",
    "authors": [
      "Jia Deng",
      "Wei Dong",
      "Richard Socher",
      "Li-Jia Li",
      "Kai Li",
      "Li Fei-Fei"
    ],
    "description": "ImageNet created a large-scale dataset with over 14 million labeled images across thousands of categories, organized hierarchically using WordNet. The annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) became the premier benchmark for computer vision. ImageNet's scale and diversity enabled the training of deep neural networks and catalyzed the deep learning revolution, particularly with AlexNet's breakthrough in 2012.",
    "citations": 58300,
    "icon": "ml/princeton.png",
    "details": "To work with ImageNet, download the ILSVRC subset (≈1.2M training images across 1,000 classes). Preprocess by resizing images so the shorter side is 256 px, then apply random 224×224 crops, horizontal flips, and mean/std normalization. Construct a data pipeline that shuffles, augments, and batches images efficiently (e.g., tf.data or PyTorch DataLoader with multiworkers). For evaluation, use center-crop or ten-crop testing. Organize labels according to WordNet synsets to maintain the semantic hierarchy. Training a modern classifier (ResNet, ViT) on ImageNet provides a standard benchmark; fine-tuning those weights on downstream tasks leverages ImageNet’s rich visual priors."
  },
  {
    "id": "Xavier",
    "period": "2010",
    "title": "Xavier/Glorot Initialization",
    "org": "Université de Montréal",
    "location": "https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf",
    "paperTitle": "Understanding the Difficulty of Training Deep Feedforward Neural Networks",
    "authors": [
      "Xavier Glorot",
      "Yoshua Bengio"
    ],
    "description": "Xavier initialization provided a principled method for initializing neural network weights to maintain consistent variance of activations and gradients across layers. By scaling initial weights based on the number of input and output connections, it prevented vanishing or exploding gradients during training. This simple but crucial technique enabled the training of much deeper networks and remains a standard practice in deep learning.",
    "citations": 25800,
    "icon": "ml/montreal.png",
    "details": "Apply Xavier (Glorot) initialization when creating dense or tanh-activated layers: draw weights W ∼ U[−√(6/(fan_in+fan_out)), √(6/(fan_in+fan_out))] for uniform, or W ∼ N(0, 2/(fan_in+fan_out)) for normal. fan_in is the number of input connections, fan_out the number of output connections. Biases typically initialize to zero. For ReLU layers, switch to He initialization (variance 2/fan_in). Most frameworks expose helpers (e.g., torch.nn.init.xavier_uniform_) but understanding the formula lets you customize for convolutions (where fan counts depend on kernel size × channels). Proper initialization stabilizes early training by keeping signal variances consistent across layers."
  },
  {
    "id": "ReLU",
    "period": "2010",
    "title": "Rectified Linear Unit (ReLU) Activation",
    "org": "University of Toronto",
    "location": "https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf",
    "paperTitle": "Rectified Linear Units Improve Restricted Boltzmann Machines",
    "authors": [
      "Vinod Nair",
      "Geoffrey E. Hinton"
    ],
    "description": "ReLU introduced a simple non-saturating activation function f(x) = max(0, x) that addressed the vanishing gradient problem of sigmoid and tanh activations. ReLU enabled faster training, reduced computational cost, and induced sparsity in neural networks. Despite its simplicity, ReLU became the default activation function for deep neural networks and enabled the training of much deeper architectures.",
    "citations": 6800,
    "icon": "ml/toronto.svg",
    "details": "To incorporate ReLU, replace sigmoid/tanh activations with f(x)=max(0,x) after linear or convolutional layers. In code, this is a single-element-wise operation (e.g., torch.nn.ReLU(inplace=True)). Combine with He initialization and learning rate schedules to ensure stable gradients. Mitigate “dying ReLU” by using LeakyReLU or ELU if many neurons output zero; batch norm also helps keep activations in a healthy range. Monitor sparsity (percentage of zeros) to confirm the intended regularizing effect. ReLU’s piecewise-linear behavior makes backprop simple: the derivative is 1 for x>0 and 0 otherwise."
  },
  {
    "id": "AlexNet",
    "period": "2012",
    "title": "AlexNet",
    "org": "University of Toronto",
    "location": "https://proceedings.neurips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf",
    "paperTitle": "ImageNet Classification with Deep Convolutional Neural Networks",
    "authors": [
      "Alex Krizhevsky",
      "Ilya Sutskever",
      "Geoffrey E. Hinton"
    ],
    "description": "AlexNet won the ImageNet 2012 competition with a significant margin, demonstrating that deep convolutional networks trained with GPUs could dramatically outperform traditional computer vision methods. The architecture combined ReLU activations, dropout regularization, data augmentation, and GPU training. AlexNet's success marked the beginning of the deep learning era and sparked intense interest in neural networks across academia and industry.",
    "citations": 150801,
    "icon": "ml/toronto.svg",
    "details": "Recreate AlexNet with five convolutional layers followed by three fully connected layers: Conv1 (11×11 stride 4, 96 filters) → LRN → max pool; Conv2 (5×5, 256 filters split across GPUs) → LRN → max pool; Conv3 (3×3, 384) → Conv4 (3×3, 384) → Conv5 (3×3, 256) → max pool; FC6 (4096) → FC7 (4096) → FC8 (1000). Use ReLU activations, dropout (p=0.5) on FC6/FC7, and overlapping pooling (stride 2). Train on ImageNet with SGD, initial lr=0.01, momentum 0.9, weight decay 5e-4, step lr decay, and heavy data augmentation (random crops, flips, color jitter). Parallelize across two GPUs by splitting filters/channel groups. Evaluate with top-1/top-5 accuracy; visualize learned kernels to ensure diverse edge/color detectors.",
    "repo": "https://github.com/akrizhevsky/cuda-convnet2"
  },
  {
    "id": "Dropout",
    "period": "2012",
    "title": "Dropout",
    "org": "University of Toronto",
    "location": "https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf",
    "paperTitle": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
    "authors": [
      "Nitish Srivastava",
      "Geoffrey Hinton",
      "Alex Krizhevsky",
      "Ilya Sutskever",
      "Ruslan Salakhutdinov"
    ],
    "description": "Dropout introduced a powerful regularization technique by randomly dropping units during training, preventing co-adaptation of features. This simple method significantly reduced overfitting in deep neural networks by training an ensemble of exponentially many sub-networks. Dropout became a standard regularization technique and enabled the training of larger networks without excessive overfitting.",
    "citations": 56200,
    "icon": "ml/toronto.svg",
    "details": "Implement dropout by zeroing activations with probability p during training: given activation vector a, sample binary mask m ∼ Bernoulli(1−p) and compute ã = (m ⊙ a)/(1−p) to keep expected magnitude constant (inverted dropout). At inference, skip masking. Apply dropout after fully connected or convolutional layers (with channel-wise masks for convs to preserve spatial structure). Tune p based on layer depth (typical values: 0.1–0.3 for convs, 0.5 for fully connected). Monitor training vs validation curves; dropout should reduce the gap without harming convergence. Combine with weight decay and batch norm for best results."
  },
  {
    "id": "Word2Vec",
    "period": "2013",
    "title": "Word2Vec",
    "org": "Google",
    "location": "https://arxiv.org/pdf/1301.3781.pdf",
    "paperTitle": "Efficient Estimation of Word Representations in Vector Space",
    "authors": [
      "Tomas Mikolov",
      "Kai Chen",
      "Greg Corrado",
      "Jeffrey Dean"
    ],
    "description": "Word2Vec introduced efficient methods (Skip-gram and CBOW) for learning dense vector representations of words from large corpora. These embeddings captured semantic and syntactic relationships, enabling vector arithmetic like 'king' - 'man' + 'woman' ≈ 'queen'. Word2Vec revolutionized natural language processing by providing a scalable way to represent words as continuous vectors, becoming foundational for modern NLP.",
    "citations": 45200,
    "icon": "ml/google.png",
    "details": "To train Skip-gram with negative sampling (SGNS): iterate over tokenized text, for each center word w_c predict surrounding context words w_o within a window k. For each positive pair (w_c, w_o), sample K negatives from the unigram distribution^0.75. Optimize log σ(u_o·v_c) + Σ log σ(−u_n·v_c), where v are input embeddings and u are output embeddings. Use subsampling to drop frequent words via P_drop(w)=1−√(t/f(w)). Implement with efficient batching (e.g., PyTorch’s EmbeddingBag) and asynchronous updates (Hogwild) for speed. After training, discard output vectors and use v as word embeddings. Evaluate via analogy tasks or downstream classifiers.",
    "repo": "https://github.com/google/word2vec"
  },
  {
    "id": "VAE",
    "period": "2013",
    "title": "Variational Autoencoder (VAE)",
    "org": "University of Amsterdam",
    "location": "https://arxiv.org/pdf/1312.6114.pdf",
    "paperTitle": "Auto-Encoding Variational Bayes",
    "authors": [
      "Diederik P. Kingma",
      "Max Welling"
    ],
    "description": "VAE introduced a probabilistic approach to learning latent representations by combining variational inference with neural networks. It learns a distribution over latent codes rather than deterministic encodings, enabling both efficient inference and generation. VAE provided a principled framework for generative modeling and became influential in unsupervised learning, representation learning, and generative AI.",
    "citations": 32100,
    "icon": "ml/amsterdam.svg",
    "details": "Implement a VAE with encoder qϕ(z|x) producing mean μ and log-variance log σ², and decoder pθ(x|z) producing reconstruction parameters. During training, sample z via reparameterization: z = μ + σ ⊙ ε, ε ∼ N(0,I), to keep gradients flowing. Optimize the ELBO: L = E_q[log pθ(x|z)] − KL[qϕ(z|x) || N(0,I)], using Monte Carlo estimates and backprop. Choose Gaussian likelihood for continuous data or Bernoulli for binary images (MNIST). Regularize by β-annealing or KL warm-up to prevent posterior collapse. Evaluate with reconstruction error and latent-space traversals; sample new data by drawing z ∼ N(0,I) and decoding."
  },
  {
    "id": "GAN",
    "period": "2014",
    "title": "Generative Adversarial Network (GAN)",
    "org": "Université de Montréal",
    "location": "https://arxiv.org/pdf/1406.2661.pdf",
    "paperTitle": "Generative Adversarial Networks",
    "authors": [
      "Ian J. Goodfellow",
      "Jean Pouget-Abadie",
      "Mehdi Mirza",
      "Bing Xu",
      "David Warde-Farley",
      "Sherjil Ozair",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "description": "GAN introduced a game-theoretic framework where a generator network learns to create realistic data by competing against a discriminator network. This adversarial training process enabled the generation of highly realistic images without requiring explicit modeling of probability distributions. GANs revolutionized generative modeling and spawned numerous applications in image synthesis, style transfer, and data augmentation.",
    "citations": 88791,
    "icon": "ml/montreal.png",
    "details": "To train a vanilla GAN: define generator G(z;θ_g) that maps latent noise z ∼ N(0,I) to data space, and discriminator D(x;θ_d) that outputs probability of x being real. Alternate optimization steps: (1) update D by maximizing log D(x_real) + log(1 − D(G(z))); (2) update G by maximizing log D(G(z)) (or minimizing log(1−D(G(z)))). Use Adam (lr=2e−4, β1=0.5) and batch normalization in G/D to stabilize. Monitor losses and generated samples to avoid mode collapse; techniques like label smoothing, gradient penalty, or WGAN objectives improve stability. Post-training, generate data by sampling fresh z and passing through G."
  },
  {
    "id": "Adam",
    "period": "2014",
    "title": "Adam Optimizer",
    "org": "OpenAI, University of Toronto",
    "location": "https://arxiv.org/pdf/1412.6980.pdf",
    "paperTitle": "Adam: A Method for Stochastic Optimization",
    "authors": [
      "Diederik P. Kingma",
      "Jimmy Ba"
    ],
    "description": "Adam combined the benefits of AdaGrad and RMSProp by computing adaptive learning rates for each parameter using estimates of first and second moments of gradients. It included bias correction terms and proved robust across a wide range of problems with minimal hyperparameter tuning. Adam became the most widely used optimizer in deep learning due to its efficiency, ease of use, and strong empirical performance.",
    "citations": 215000,
    "icon": "ml/openai.svg",
    "details": "Implement Adam updates as follows for parameter θ: maintain m_t (first moment) and v_t (second moment). Given gradient g_t, update m_t = β1 m_{t−1} + (1−β1) g_t, v_t = β2 v_{t−1} + (1−β2) g_t^2. Bias-correct: m̂_t = m_t / (1−β1^t), v̂_t = v_t / (1−β2^t). Apply parameter update θ ← θ − α · m̂_t / (√(v̂_t) + ε), with defaults α=1e−3, β1=0.9, β2=0.999, ε=1e−8. Implement weight decay separately (AdamW) to avoid coupling decay with adaptive steps. Monitor training by checking for divergence when β1 or β2 are set too high, and consider decoupled learning-rate schedulers or warm restarts for large-scale training."
  },
  {
    "id": "Seq2Seq",
    "period": "2014",
    "title": "Sequence-to-Sequence Learning",
    "org": "Google",
    "location": "https://arxiv.org/pdf/1409.3215.pdf",
    "paperTitle": "Sequence to Sequence Learning with Neural Networks",
    "authors": [
      "Ilya Sutskever",
      "Oriol Vinyals",
      "Quoc V. Le"
    ],
    "description": "Seq2Seq introduced an end-to-end framework for sequence transduction using an encoder-decoder architecture with LSTMs. The encoder maps variable-length input sequences to fixed-size representations, which the decoder transforms into variable-length output sequences. This architecture unified many NLP tasks under a single framework and achieved breakthrough results in machine translation, establishing neural approaches as state-of-the-art.",
    "citations": 26800,
    "icon": "ml/google.png",
    "details": "Build a Seq2Seq translator with LSTM encoder-decoder: embed input tokens, pass through a bidirectional LSTM to produce hidden states. Use the final encoder state to initialize the decoder LSTM. During training, apply teacher forcing: feed ground-truth previous token to the decoder, project hidden state to vocabulary logits, and minimize cross-entropy. At inference, decode autoregressively with greedy or beam search. Handle variable lengths via padding + masking, and add attention (Bahdanau/Luong) to improve long-sentence performance. Optimizers like Adam with scheduled learning rates and label smoothing help stabilize training on large corpora.",
    "repo": "https://github.com/google/seq2seq"
  },
  {
    "id": "Bahdanau_Attention",
    "period": "2014",
    "title": "Attention Mechanism",
    "org": "Université de Montréal",
    "location": "https://arxiv.org/pdf/1409.0473.pdf",
    "paperTitle": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "authors": [
      "Dzmitry Bahdanau",
      "Kyunghyun Cho",
      "Yoshua Bengio"
    ],
    "description": "Bahdanau attention addressed the bottleneck in sequence-to-sequence models by allowing the decoder to focus on different parts of the input sequence at each decoding step. This attention mechanism computed context vectors as weighted sums of encoder hidden states, where weights were learned based on relevance. Attention became a fundamental building block of modern NLP systems and directly inspired the transformer architecture.",
    "citations": 47200,
    "icon": "ml/montreal.png",
    "details": "To add additive (Bahdanau) attention, compute alignment scores e_t,i = vᵗ tanh(W₁h_{t−1} + W₂hᵢ) between decoder hidden state h_{t−1} and each encoder hidden state hᵢ. Normalize via softmax α_t,i = softmax(e_t,i), then form context vector c_t = Σ α_t,i hᵢ. Concatenate c_t with decoder input (or hidden state) before predicting the next token. Implement efficiently by stacking encoder states and using batched matrix ops. Train end-to-end with Seq2Seq loss; inspect heatmaps of α to ensure the decoder attends to relevant source positions. This mechanism alleviates the fixed-length bottleneck and improves translations of long sentences."
  },
  {
    "id": "GloVe",
    "period": "2014",
    "title": "GloVe Word Embeddings",
    "org": "Stanford University",
    "location": "https://nlp.stanford.edu/pubs/glove.pdf",
    "paperTitle": "GloVe: Global Vectors for Word Representation",
    "authors": [
      "Jeffrey Pennington",
      "Richard Socher",
      "Christopher D. Manning"
    ],
    "description": "GloVe combined global matrix factorization with local context window methods for learning word embeddings. It trained on aggregated word-word co-occurrence statistics to produce vectors with meaningful linear substructures. GloVe provided an alternative to Word2Vec with strong performance on word analogy and similarity tasks, and its pre-trained vectors became widely used in NLP applications.",
    "citations": 40500,
    "icon": "ml/stanford.png",
    "details": "Implement GloVe by constructing a sparse co-occurrence matrix X where X_ij counts how often word j appears in the context of word i within a window. Optimize the weighted least-squares objective: Σ f(X_ij) (w_iᵗ w̃_j + b_i + b̃_j − log X_ij)^2, where f(x) = min((x/x_max)^α, 1). Typical hyperparameters: x_max = 100, α = 0.75, embedding dimension 100–300. Use AdaGrad or Adam with mini-batches over non-zero entries. After training, sum word and context vectors (w_i + w̃_i) to form the final embedding. Evaluate on word analogy tasks by checking that vector differences correspond to semantic relations.",
    "repo": "https://github.com/stanfordnlp/GloVe"
  },
  {
    "id": "Neural_Turing_Machine",
    "period": "2014",
    "title": "Neural Turing Machine",
    "org": "Google DeepMind",
    "location": "https://arxiv.org/pdf/1410.5401.pdf",
    "paperTitle": "Neural Turing Machines",
    "authors": [
      "Alex Graves",
      "Greg Wayne",
      "Ivo Danihelka"
    ],
    "description": "Neural Turing Machines extended neural networks by coupling them to external memory resources accessed through attention mechanisms. The entire system was differentiable end-to-end, allowing gradient-based training. NTMs demonstrated that neural networks could learn simple algorithms like copying, sorting, and associative recall from examples alone, showing that neural networks could exhibit more algorithmic and programmable behavior.",
    "citations": 3850,
    "icon": "ml/deepmind.svg",
    "details": "To experiment with NTMs, implement a controller (LSTM) connected to an external memory matrix M. Read heads produce attention weights over memory via content-based addressing (similarity between key vector and memory rows) optionally shifted by learned convolutional kernels for location-based addressing. The read vector r is a weighted sum of memory rows. Write heads emit erase and add vectors to modify memory: M ← M ⊙ (1 − w ⊗ e) + w ⊗ a. At each timestep, concatenate controller output with read vectors to produce task outputs. Train via gradient descent on algorithmic tasks (copy, repeat-copy). Monitor attention distributions to ensure the model learns consistent read/write patterns."
  },
  {
    "id": "BatchNorm",
    "period": "2015",
    "title": "Batch Normalization",
    "org": "Google",
    "location": "https://arxiv.org/pdf/1502.03167.pdf",
    "paperTitle": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
    "authors": [
      "Sergey Ioffe",
      "Christian Szegedy"
    ],
    "description": "Batch Normalization normalized layer inputs across mini-batches, stabilizing training by reducing internal covariate shift. It enabled much higher learning rates, reduced sensitivity to initialization, and acted as a regularizer. Batch normalization dramatically accelerated training and became a standard component in deep networks, enabling the training of very deep architectures that were previously difficult to optimize.",
    "citations": 72400,
    "icon": "ml/google.png",
    "details": "For each batch, compute mean μ_B and variance σ²_B of activations for a given feature. Normalize: ˜ x = (x − μ_B) / √(σ²_B + ε), then scale and shift with learnable γ, β: y = γ ˜ x + β. Insert BN between linear/conv layers and nonlinearities (typically before ReLU). During inference, use running averages of μ_B and σ²_B accumulated during training. BN allows higher learning rates and reduces dependency on initialization. When batch sizes are small, switch to LayerNorm or GroupNorm. Remember to disable dropout before BN in the layer order (Conv → BN → ReLU)."
  },
  {
    "id": "ResNet",
    "period": "2015",
    "title": "Residual Networks (ResNet)",
    "org": "Microsoft Research",
    "location": "https://arxiv.org/pdf/1512.03385.pdf",
    "paperTitle": "Deep Residual Learning for Image Recognition",
    "authors": [
      "Kaiming He",
      "Xiangyu Zhang",
      "Shaoqing Ren",
      "Jian Sun"
    ],
    "description": "ResNet introduced skip connections that allowed gradients to flow directly through networks by learning residual mappings. This simple architectural change enabled the training of networks with hundreds or even thousands of layers without degradation problems. ResNet won ImageNet 2015 and demonstrated that very deep networks could be effectively trained, fundamentally changing how we design neural network architectures.",
    "citations": 298566,
    "icon": "ml/microsoft.png",
    "details": "Implement ResNet-50 using bottleneck blocks: 1×1 conv (reduce channels) → 3×3 conv → 1×1 conv (expand), each followed by BatchNorm + ReLU. Add identity shortcuts so block output is F(x)+x; when spatial or channel dimensions change, apply a 1×1 projection to the shortcut. Arrange blocks in stages {3,4,6,3} with stride-2 downsampling at stage boundaries. Train on ImageNet with SGD (lr=0.1, momentum 0.9, weight decay 1e−4) and step/cosine LR schedules, using data augmentation (random crops, flips, color jitter). Residual connections preserve gradient flow, enabling very deep stacks—monitor training to ensure earlier layers remain active and validation accuracy surpasses non-residual baselines.",
    "repo": "https://github.com/KaimingHe/deep-residual-networks"
  },
  {
    "id": "Luong_Attention",
    "period": "2015",
    "title": "Luong Attention (Global and Local Attention)",
    "org": "Stanford University",
    "location": "https://aclanthology.org/D15-1166.pdf",
    "paperTitle": "Effective Approaches to Attention-based Neural Machine Translation",
    "authors": [
      "Minh-Thang Luong",
      "Hieu Pham",
      "Christopher D. Manning"
    ],
    "description": "Luong attention introduced two complementary attention mechanisms for neural machine translation: global attention, which attends to all source words, and local attention, which focuses on a subset of source positions. The paper also proposed multiplicative (dot-product) attention as a simpler alternative to additive attention. These mechanisms achieved significant improvements over non-attentional systems, with the local attention approach gaining 5.0 BLEU points. The work established a new state-of-the-art on WMT'15 English-German translation and influenced subsequent attention designs in transformers.",
    "citations": 14200,
    "icon": "ml/stanford.png",
    "details": "To implement Luong attention, compute alignment scores between decoder hidden state hₜ and encoder hidden states h̄ₛ. For global attention, use score(hₜ, h̄ₛ) via one of: dot product (hₜᵀh̄ₛ), general (hₜᵀWₐh̄ₛ), or concat (vₐᵀtanh(Wₐ[hₜ;h̄ₛ])). Normalize scores with softmax to get attention weights αₜ, compute context vector cₜ = Σαₜₛh̄ₛ, then concatenate [cₜ;hₜ] and project through a feedforward layer to get the attentional hidden state h̃ₜ. For local attention, first predict an aligned position pₜ (either monotonic or learned), then attend only to positions in window [pₜ−D, pₜ+D] using Gaussian-weighted scores. Input-feeding connects h̃ₜ₋₁ to the decoder input at step t, making the model aware of previous attention decisions. Train end-to-end with cross-entropy loss on parallel corpora."
  },
  {
    "id": "Layer_Normalization",
    "period": "2016",
    "title": "Layer Normalization",
    "org": "University of Toronto",
    "location": "https://arxiv.org/pdf/1607.06450.pdf",
    "paperTitle": "Layer Normalization",
    "authors": [
      "Jimmy Lei Ba",
      "Jamie Ryan Kiros",
      "Geoffrey E. Hinton"
    ],
    "description": "Layer Normalization normalized inputs across features for each example independently, unlike batch normalization which normalized across the batch dimension. This made it particularly effective for recurrent neural networks and sequences of varying length. Layer normalization stabilized hidden state dynamics in RNNs and later became the standard normalization technique in transformer architectures.",
    "citations": 20100,
    "icon": "ml/toronto.svg",
    "details": "For a hidden vector h with H features, compute μ = (1/H) Σ h_i and σ² = (1/H) Σ (h_i − μ)^2 per sample. Normalize as ˜ h = (h − μ) / √(σ² + ε), then learn affine parameters γ, β so output y = γ ⊙ ˜ h + β. Apply LayerNorm to each timestep independently in RNNs or to the last dimension in transformers (often before attention/MLP sublayers in a pre-LN architecture). Since statistics are sample-wise, LN works with batch size 1 and avoids running averages. Implementation in PyTorch: nn.LayerNorm(hidden_dim). Tune ε to avoid numerical issues when σ is small."
  },
  {
    "id": "BPE",
    "period": "2016",
    "title": "Subword Units (BPE): Solving the Rare Word Problem",
    "org": "University of Edinburgh",
    "location": "https://arxiv.org/pdf/1508.07909.pdf",
    "paperTitle": "Neural Machine Translation of Rare Words with Subword Units",
    "authors": [
      "Rico Sennrich",
      "Barry Haddow",
      "Alexandra Birch"
    ],
    "description": "Byte-Pair Encoding (BPE) adapted a data compression algorithm for neural machine translation, enabling open-vocabulary learning by breaking words into subword units. This solved the rare word problem by representing infrequent words as sequences of common subwords. BPE became the standard tokenization approach for language models, enabling models to handle any word while maintaining reasonable vocabulary sizes, and is used in GPT, BERT, and most modern LLMs.",
    "citations": 14800,
    "icon": "ml/edinburgh.png",
    "details": "To train BPE, initialize the vocabulary with all characters (or bytes) and count symbol pair frequencies in the corpus. Iteratively merge the most frequent adjacent pair (e.g., 't'+'h'→'th'), add the new token to the vocabulary, and update the corpus by replacing occurrences of that pair. Repeat until reaching the desired vocab size (typically 30k–50k). At tokenization time, greedily apply the learned merges from most to least frequent to segment new words into subwords. For neural models, replace words with their subword sequences before feeding them into embeddings. Libraries like SentencePiece or Hugging Face tokenizers expose BPE training/payment APIs; ensure consistent pre-processing (lowercasing, normalization) between training and inference.",
    "repo": "https://github.com/rsennrich/subword-nmt"
  },
  {
    "id": "KV_Memory_Networks",
    "period": "2016",
    "title": "Key-Value Memory Networks",
    "org": "Facebook AI Research, Carnegie Mellon University",
    "location": "https://aclanthology.org/D16-1147.pdf",
    "paperTitle": "Key-Value Memory Networks for Directly Reading Documents",
    "authors": [
      "Alexander H. Miller",
      "Adam Fisch",
      "Jesse Dodge",
      "Amir-Hossein Karimi",
      "Antoine Bordes",
      "Jason Weston"
    ],
    "description": "Key-Value Memory Networks introduced a memory architecture that separates keys (used for addressing) from values (used for reading), enabling more effective question answering by directly reading documents. This separation allowed the model to use different encodings for matching queries to memory slots versus returning information, significantly improving performance on knowledge base and document-based QA tasks. The architecture influenced subsequent memory-augmented networks and retrieval-augmented generation systems.",
    "citations": 1318,
    "icon": "ml/meta.svg",
    "details": "Implement Key-Value Memory Networks by storing documents as (key, value) pairs in memory. Keys are encoded representations optimized for matching against the query, while values contain the information to be retrieved. Given a query q, compute attention weights over keys: α_i = softmax(q · k_i), then read from memory by computing the weighted sum of values: o = Σ α_i v_i. The output is combined with the query for answer prediction. For document QA, keys might encode article titles or sentences while values encode the full text. Train end-to-end with supervision on QA pairs. Extend to multiple hops by using the output o to requery memory iteratively, refining the answer. Evaluate on WikiQA, WikiMovies, or bAbI tasks."
  },
  {
    "id": "Transformer",
    "period": "2017",
    "title": "Transformer Architecture",
    "org": "Google",
    "location": "https://arxiv.org/pdf/1706.03762.pdf",
    "paperTitle": "Attention Is All You Need",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Łukasz Kaiser",
      "Illia Polosukhin"
    ],
    "description": "The Transformer replaced recurrence and convolutions entirely with self-attention mechanisms, processing sequences in parallel rather than sequentially. It introduced multi-head attention, positional encodings, and a feedforward encoder-decoder structure. The Transformer achieved state-of-the-art translation results while being more parallelizable and requiring significantly less training time. This architecture became the foundation for modern large language models and revolutionized NLP.",
    "citations": 209982,
    "icon": "ml/google.png",
    "details": "Implement a Transformer block with multi-head attention and position-wise feedforward layers. For attention, project inputs to Q, K, V, split into h heads, compute softmax(QKᵗ/√d_k)V with masking for padding/causal constraints, then concatenate heads and project back. Add residual connections and LayerNorm around attention and feedforward sublayers (pre-LN: LN → sublayer → residual). Encode sequence order with sinusoidal or learned positional embeddings. Stack encoder blocks; decoder blocks include masked self-attention plus encoder–decoder attention. Train with Adam (β₁=0.9, β₂=0.98) and warmup/inverse-sqrt LR schedule, apply label smoothing and dropout. Use beam search during inference.",
    "repo": "https://github.com/tensorflow/tensor2tensor"
  },
  {
    "id": "RLHF",
    "period": "2017",
    "title": "Reinforcement Learning from Human Feedback (RLHF)",
    "org": "OpenAI, UC Berkeley, DeepMind",
    "location": "https://arxiv.org/pdf/1706.03741.pdf",
    "paperTitle": "Deep Reinforcement Learning from Human Preferences",
    "authors": [
      "Paul Christiano",
      "Jan Leike",
      "Tom Brown",
      "Miljan Martic",
      "Shane Legg",
      "Dario Amodei"
    ],
    "description": "RLHF introduced a method for training RL agents using human preference comparisons rather than hand-crafted reward functions. Humans compared pairs of trajectory segments, and a reward model was trained to predict preferences. This reward model then guided policy optimization. RLHF scaled preference-based learning to complex tasks and later became crucial for aligning large language models with human values and intentions.",
    "citations": 3250,
    "icon": "ml/openai.svg",
    "details": "RLHF pipeline: (1) Collect human preference pairs between model outputs or trajectories. (2) Train a reward model Rϕ(x,y) using logistic regression on preference data: maximize log σ(Rϕ(y_a) − Rϕ(y_b)). (3) Fine-tune the base policy π using RL (often PPO) with reward Rϕ minus a KL penalty toward a reference policy to prevent drift. Implement PPO with clipped objective, advantages from GAE, and KL coefficient tuned to maintain target divergence. Evaluate alignment via held-out preference evaluations and safety audits."
  },
  {
    "id": "MoE",
    "period": "2017",
    "title": "Sparsely-Gated Mixture of Experts",
    "org": "Google",
    "location": "https://arxiv.org/pdf/1701.06538.pdf",
    "paperTitle": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
    "authors": [
      "Noam Shazeer",
      "Azalia Mirhoseini",
      "Krzysztof Maziarz",
      "Andy Davis",
      "Quoc Le",
      "Geoffrey Hinton",
      "Jeff Dean"
    ],
    "description": "Mixture of Experts introduced conditional computation where a gating network routes each input to a sparse subset of expert sub-networks. This enabled training models with orders of magnitude more parameters without proportional increases in computation. MoE demonstrated that model capacity could be dramatically increased through sparsity, achieving state-of-the-art results in language modeling and translation. This approach later influenced large-scale models like GPT-4.",
    "citations": 3050,
    "icon": "ml/google.png",
    "details": "Build a sparsely gated MoE layer by defining E experts (feedforward networks) and a gating network producing logits g. Select top-k experts per token (k=1 or 2) using softmax probabilities and optionally add noise for exploration. Route inputs to chosen experts, weight their outputs by gate probabilities, and sum. Include load-balancing losses to keep expert utilization even. Implement with efficient dispatch/combination ops (e.g., Switch Transformer routing). Train end-to-end with standard optimizers, monitor expert load metrics, and adjust k or noise to avoid expert collapse."
  },
  {
    "id": "PPO",
    "period": "2017",
    "title": "Proximal Policy Optimization (PPO)",
    "org": "OpenAI",
    "location": "https://arxiv.org/pdf/1707.06347.pdf",
    "paperTitle": "Proximal Policy Optimization Algorithms",
    "authors": [
      "John Schulman",
      "Filip Wolski",
      "Prafulla Dhariwal",
      "Alec Radford",
      "Oleg Klimov"
    ],
    "description": "PPO introduced a simpler and more stable policy gradient method by clipping the objective function to prevent excessively large policy updates. It combined the benefits of trust region methods with the simplicity of first-order optimization. PPO became the most widely used reinforcement learning algorithm due to its robustness, ease of implementation, and strong empirical performance across diverse tasks.",
    "citations": 16800,
    "icon": "ml/openai.svg",
    "details": "Implement PPO with clipped surrogate objective: L = E[min(r_t(θ)Â_t, clip(r_t(θ), 1−ε, 1+ε)Â_t)], where r_t is the probability ratio and Â_t Generalized Advantage Estimation. Collect trajectories with π_old, compute returns/advantages, then run multiple epochs of mini-batch SGD (Adam lr≈3e−4) updating both policy and value heads. Include value loss and entropy bonus; monitor KL divergence to adjust learning rate or early-stop. Works for continuous control (Gaussian policies with tanh outputs) and discrete actions. Use normalized advantages and gradient clipping for stability.",
    "repo": "https://github.com/openai/baselines"
  },
  {
    "id": "ELMo",
    "period": "2018",
    "title": "ELMo (Embeddings from Language Models)",
    "org": "Allen Institute for AI, University of Washington",
    "location": "https://arxiv.org/pdf/1802.05365.pdf",
    "paperTitle": "Deep Contextualized Word Representations",
    "authors": [
      "Matthew E. Peters",
      "Mark Neumann",
      "Mohit Iyyer",
      "Matt Gardner",
      "Christopher Clark",
      "Kenton Lee",
      "Luke Zettlemoyer"
    ],
    "description": "ELMo generated context-dependent word representations by using bidirectional LSTMs trained as language models. Unlike static embeddings, ELMo representations varied based on context, capturing polysemy and complex linguistic features. ELMo demonstrated the power of pre-training and fine-tuning, significantly improving performance across diverse NLP tasks. It was a crucial step toward modern contextualized language models and transfer learning in NLP.",
    "citations": 17800,
    "icon": "ml/allen_ai.svg",
    "details": "Train a bidirectional 2-layer LSTM language model on large text (e.g., 1B Word Benchmark). For each token, concatenate forward and backward hidden states from all layers, then learn task-specific scalar weights to combine them. During downstream training, keep LM weights fixed or fine-tune lightly while adding the weighted ELMo embeddings to the model input (or intermediate layers). Implementation tips: apply character CNNs to build robust token embeddings, use dropout/variational dropout in LSTMs, and train with sampled softmax for efficiency. Evaluate by plugging ELMo into tagging, QA, or classification models and measuring gains.",
    "repo": "https://github.com/allenai/bilm-tf"
  },
  {
    "id": "GPT",
    "period": "2018",
    "title": "GPT (Generative Pre-Training)",
    "org": "OpenAI",
    "location": "https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf",
    "paperTitle": "Improving Language Understanding by Generative Pre-Training",
    "authors": [
      "Alec Radford",
      "Karthik Narasimhan",
      "Tim Salimans",
      "Ilya Sutskever"
    ],
    "description": "GPT introduced a two-stage approach: unsupervised pre-training of a transformer language model on large text corpora, followed by supervised fine-tuning on specific tasks. This demonstrated that language models could learn general representations useful across many tasks. GPT showed that pre-training could significantly reduce the labeled data required for downstream tasks, establishing the pre-train-then-fine-tune paradigm that dominated subsequent NLP research.",
    "citations": 6100,
    "icon": "ml/openai.svg",
    "details": "To replicate GPT, train a decoder-only Transformer with masked self-attention on large text (e.g., BookCorpus). Use byte pair encoding, 12 layers, hidden size 768, 12 heads, context length 512, GELU activations, and learned positional embeddings. Optimize with Adam (β1=0.9, β2=0.95), LR warmup + cosine decay, and layer-wise LR decay. Fine-tune on downstream tasks by prepending task-specific prompts and continuing supervised training with small LR. Evaluation uses perplexity for pretraining and task-specific metrics (accuracy, F1) during fine-tuning.",
    "repo": "https://github.com/openai/finetune-transformer-lm"
  },
  {
    "id": "BERT",
    "period": "2018",
    "title": "BERT (Bidirectional Encoder Representations from Transformers)",
    "org": "Google",
    "location": "https://arxiv.org/pdf/1810.04805.pdf",
    "paperTitle": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "authors": [
      "Jacob Devlin",
      "Ming-Wei Chang",
      "Kenton Lee",
      "Kristina Toutanova"
    ],
    "description": "BERT pre-trained bidirectional transformers using masked language modeling and next sentence prediction. Unlike previous unidirectional models, BERT jointly conditioned on both left and right context in all layers. BERT achieved state-of-the-art results across eleven NLP tasks and demonstrated that deeply bidirectional pre-training was crucial for language understanding. BERT became the foundation for numerous downstream applications and variants.",
    "citations": 152370,
    "icon": "ml/google.png",
    "details": "Pretrain BERT by randomly masking 15% of tokens (80% [MASK], 10% random token, 10% unchanged) and predicting them with a Transformer encoder (L=12 or 24). Add Next Sentence Prediction by classifying whether sentence B follows sentence A using [CLS] token representation. Use WordPiece tokenization, segment + position embeddings, GELU activations, dropout 0.1, Adam optimizer (lr=1e−4) with warmup over 10k steps and linear decay. Fine-tune by adding task-specific heads on [CLS] (classification) or token embeddings (QA, tagging) and training for a few epochs with small LR.",
    "repo": "https://github.com/google-research/bert"
  },
  {
    "id": "Mixed_Precision_Training",
    "period": "2018",
    "title": "Mixed Precision Training",
    "org": "NVIDIA",
    "location": "https://arxiv.org/pdf/1710.03740.pdf",
    "paperTitle": "Mixed Precision Training",
    "authors": [
      "Paulius Micikevicius",
      "Sharan Narang",
      "Jonah Alben",
      "Gregory Diamos",
      "Erich Elsen",
      "David Garcia",
      "Boris Ginsburg",
      "Michael Houston",
      "Olaf Klauser",
      "Andrew Kraljevic",
      "Chris Paine",
      "Naveen Satish",
      "Michael Wu"
    ],
    "description": "Micikevicius et al. showed how to safely train deep networks using half-precision (FP16) arithmetic while preserving full-precision accuracy. By keeping FP32 master weights, accumulating gradients in FP32, and using loss scaling to avoid underflow, they demonstrated 2–3× speedups on NVIDIA Tensor Cores without sacrificing convergence. Mixed precision became the standard recipe for large-scale transformer training, enabling today's models to fit within GPU memory budgets.",
    "citations": 4700,
    "icon": "ml/nvidia.png",
    "details": "To implement mixed precision, cast activations, gradients, and matrix multiplications to FP16/BF16 while maintaining FP32 master weights. Use dynamic loss scaling: multiply the loss by a scale factor S, backpropagate in FP16, check for overflow (NaNs/infs), and if none occur divide gradients by S before the optimizer step; otherwise skip the update and reduce S. Retain FP32 copies of weights and optimizer states (e.g., Adam’s moments) and update them in full precision, then cast the updated weights back to FP16 for the forward pass. Combine with Tensor Core-optimized kernels, gradient accumulation, and ZeRO-style sharding for maximal throughput. Frameworks such as NVIDIA Apex AMP and PyTorch’s torch.cuda.amp automate these steps—enable autocast for forward passes and wrap the backward step with GradScaler to manage loss scaling.",
    "repo": "https://github.com/NVIDIA/apex"
  },
  {
    "id": "GPT-2",
    "period": "2019",
    "title": "GPT-2",
    "org": "OpenAI",
    "location": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
    "paperTitle": "Language Models are Unsupervised Multitask Learners",
    "authors": [
      "Alec Radford",
      "Jeffrey Wu",
      "Rewon Child",
      "David Luan",
      "Dario Amodei",
      "Ilya Sutskever"
    ],
    "description": "GPT-2 scaled up the original GPT to 1.5 billion parameters and trained on a larger, more diverse dataset. It demonstrated that language models could perform many tasks zero-shot without fine-tuning by simply conditioning on appropriate prompts. GPT-2 showed strong performance on diverse tasks including translation, summarization, and question answering, suggesting that with sufficient scale and data, language models naturally learn multitask capabilities.",
    "citations": 12400,
    "icon": "ml/openai.svg",
    "details": "GPT-2 extends GPT with larger decoder-only Transformers (117M–1.5B parameters) trained on WebText (45M web pages). Use byte-level BPE, context length 1024, GELU, residual/LN structure, and Adam with LR warmup + cosine decay. During inference, perform nucleus (top-p) or top-k sampling with temperature scaling to control creativity; apply repetition penalties to avoid loops. Evaluate zero-shot by crafting prompts for downstream tasks. Fine-tuning can be done via supervised objectives, but GPT-2 highlighted the power of zero-shot prompting on diverse benchmarks.",
    "repo": "https://github.com/openai/gpt-2"
  },
  {
    "id": "T5",
    "period": "2019",
    "title": "T5 (Text-to-Text Transfer Transformer)",
    "org": "Google",
    "location": "https://arxiv.org/pdf/1910.10683.pdf",
    "paperTitle": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "authors": [
      "Colin Raffel",
      "Noam Shazeer",
      "Adam Roberts",
      "Katherine Lee",
      "Sharan Narang",
      "Michael Matena",
      "Yanqi Zhou",
      "Wei Li",
      "Peter J. Liu"
    ],
    "description": "T5 unified all NLP tasks into a text-to-text format where both inputs and outputs are text strings. It systematically explored transfer learning techniques including pre-training objectives, architectures, datasets, and fine-tuning methods. T5's encoder-decoder architecture and comprehensive evaluation provided insights into what makes transfer learning effective. The unified framework simplified multi-task learning and became influential for instruction-following models.",
    "citations": 21500,
    "icon": "ml/google.png",
    "details": "Convert every supervised task into a text prompt (e.g., “translate English to German: ...”) and train a Transformer encoder-decoder with relative position embeddings using span-corruption: mask contiguous spans and have the decoder reconstruct them. Use Adafactor optimizer, learning-rate warmup then inverse-square decay, dropout 0.1, and gradient checkpointing for large models. Fine-tune by continuing text-to-text training on task data, optionally with adapters or LoRA for efficiency. Evaluate outputs with beam search and task-specific metrics.",
    "repo": "https://github.com/google-research/text-to-text-transfer-transformer"
  },
  {
    "id": "Bitter_Lesson",
    "period": "2019",
    "title": "The Bitter Lesson",
    "org": "University of Alberta, DeepMind",
    "location": "http://www.incompleteideas.net/IncIdeas/BitterLesson.html",
    "paperTitle": "The Bitter Lesson (Essay)",
    "authors": [
      "Richard Sutton"
    ],
    "description": "The Bitter Lesson essay argued that general methods leveraging computation consistently outperform approaches that rely on human knowledge in the long run. Sutton observed that search and learning, when given sufficient computation, surpass hand-crafted features and domain expertise. This philosophical perspective influenced the field to focus on scalable learning methods rather than encoding human knowledge, providing intellectual foundation for the scaling paradigm in modern AI.",
    "citations": 520,
    "icon": "ml/alberta.png",
    "details": "Although conceptual, the Bitter Lesson guides implementation strategy: prioritize scalable compute-driven methods over manual heuristics. When tackling a problem, evaluate whether additional data, larger models, or automated search can outperform hand-crafted rules. Build pipelines that make scaling easy (efficient data ingestion, distributed training). Benchmark heuristic approaches against learned ones to quantify improvements as compute grows, and favor architectures that naturally benefit from scale (self-play, gradient-based learning)."
  },
  {
    "id": "Scaling_Laws",
    "period": "2020",
    "title": "Scaling Laws for Neural Language Models",
    "org": "OpenAI",
    "location": "https://arxiv.org/pdf/2001.08361.pdf",
    "paperTitle": "Scaling Laws for Neural Language Models",
    "authors": [
      "Jared Kaplan",
      "Sam McCandlish",
      "Tom Henighan",
      "Tom B. Brown",
      "Benjamin Chess",
      "Rewon Child",
      "Scott Gray",
      "Alec Radford",
      "Jeffrey Wu",
      "Dario Amodei"
    ],
    "description": "This work empirically demonstrated that language model performance scales as power-laws with model size, dataset size, and compute budget. The research showed predictable relationships between these factors and suggested optimal allocation strategies. These scaling laws provided quantitative guidance for training large models and predicted that simply scaling up models would continue to yield improvements, influencing subsequent investment in large-scale model development.",
    "citations": 5400,
    "icon": "ml/openai.svg",
    "details": "Apply scaling laws by running models across a grid of parameter counts, dataset sizes, and compute budgets, logging validation loss. Fit power-law curves (log-loss vs. log-parameters) to estimate exponents. Use the fitted curves to predict optimal allocation of compute between model size and data (e.g., N ∝ C^α). Update fits when architecture/data change. This methodology lets you budget future training runs and estimate diminishing returns before committing resources."
  },
  {
    "id": "GPT-3",
    "period": "2020",
    "title": "GPT-3",
    "org": "OpenAI",
    "location": "https://arxiv.org/pdf/2005.14165.pdf",
    "paperTitle": "Language Models are Few-Shot Learners",
    "authors": [
      "Tom B. Brown",
      "Benjamin Mann",
      "Nick Ryder",
      "Melanie Subbiah",
      "Jared Kaplan",
      "Prafulla Dhariwal",
      "Arvind Neelakantan",
      "Pranav Shyam",
      "Girish Sastry",
      "Amanda Askell",
      "Sandhini Agarwal",
      "Ariel Herbert-Voss",
      "Gretchen Krueger",
      "Tom Henighan",
      "Rewon Child",
      "Aditya Ramesh",
      "Daniel M. Ziegler",
      "Jeffrey Wu",
      "Clemens Winter",
      "Christopher Hesse",
      "Mark Chen",
      "Eric Sigler",
      "Mateusz Litwin",
      "Scott Gray",
      "Benjamin Chess",
      "Jack Clark",
      "Christopher Berner",
      "Sam McCandlish",
      "Alec Radford",
      "Ilya Sutskever",
      "Dario Amodei"
    ],
    "description": "GPT-3 scaled transformers to 175 billion parameters, demonstrating that language models could perform diverse tasks with few-shot, one-shot, or zero-shot learning from prompts alone. It showed impressive performance on translation, question-answering, arithmetic, and novel word usage without gradient updates. GPT-3 revealed that with sufficient scale, language models develop broad capabilities and sparked widespread interest in large language models and prompt engineering.",
    "citations": 30200,
    "icon": "ml/openai.svg",
    "details": "GPT-3 uses a 96-layer decoder-only Transformer (hidden size 12,288, 96 heads) trained on ~300B tokens. Implement via distributed training: tensor/pipeline parallelism, ZeRO sharding, mixed precision (BF16), and gradient checkpointing. Use Adam with lr warmup then cosine decay, weight decay 0.01, and batch sizes in the millions of tokens. For inference, craft prompts with few-shot demonstrations and decode using top-k/top-p sampling. Evaluate zero-shot/few-shot performance on diverse benchmarks (SuperGLUE, LAMBADA) and run safety/bias audits."
  },
  {
    "id": "ZeRO",
    "period": "2020",
    "title": "ZeRO (Zero Redundancy Optimizer)",
    "org": "Microsoft",
    "location": "https://arxiv.org/pdf/1910.02054.pdf",
    "paperTitle": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models",
    "authors": [
      "Samyam Rajbhandari",
      "Jeff Rasley",
      "Olatunji Ruwase",
      "Yuxiong He"
    ],
    "description": "ZeRO eliminated memory redundancies in data-parallel distributed training by partitioning optimizer states, gradients, and parameters across devices rather than replicating them. ZeRO enabled training models with trillions of parameters by dramatically reducing per-device memory requirements while maintaining computational efficiency. This optimization became crucial for training large language models and is implemented in DeepSpeed, enabling the scale of models like GPT-3 and beyond.",
    "citations": 3200,
    "icon": "ml/microsoft.png",
    "details": "Implement ZeRO in stages: Stage 1 shards optimizer states (momentum/variance) across ranks; Stage 2 additionally shards gradients; Stage 3 partitions parameters themselves. Use DeepSpeed or FairScale to manage communication (reduce-scatter/all-gather) during forward/backward passes. Enable offloading to CPU/NVMe for very large models, and combine with activation checkpointing + mixed precision. Monitor memory by inspecting per-rank statistics; ZeRO lets you scale model size roughly linearly with number of GPUs without replicating optimizer state on each device.",
    "repo": "https://github.com/microsoft/DeepSpeed"
  },
  {
    "id": "RoPE",
    "period": "2021",
    "title": "RoFormer: Rotary Position Embedding (RoPE)",
    "org": "Zhuiyi Technology",
    "location": "https://arxiv.org/pdf/2104.09864.pdf",
    "paperTitle": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "authors": [
      "Jianlin Su",
      "Yu Lu",
      "Shengfeng Pan",
      "Ahmed Murtadha",
      "Bo Wen",
      "Yunfeng Liu"
    ],
    "description": "Rotary Position Embedding (RoPE) encodes position information by rotating word embeddings based on their absolute positions, while naturally encoding relative position information through the rotation properties. RoPE provided better extrapolation to longer sequences than previous position encoding methods while being computationally efficient. It was adopted by influential models including PaLM, LLaMA, and many other modern LLMs, becoming a preferred position encoding technique.",
    "citations": 2800,
    "icon": "ml/zhuiyi.png",
    "details": "To add RoPE, pair embedding dimensions into complex numbers and rotate each pair by angle θ = pos · base^{-2i/d}, where i indexes the pair and base is typically 10,000. Implementation: for queries/keys in attention, reshape to (seq, heads, d/2, 2) and apply rotation matrix [[cosθ, −sinθ],[sinθ, cosθ]]. This injects relative positional info without extra parameters. For long-context extrapolation, scale θ via linear interpolation or NTK scaling. Libraries like xformers and transformers provide rotary_embedding APIs—enable by setting rotary_emb_fraction and applying to Q/K before dot products.",
    "repo": "https://github.com/ZhuiyiTechnology/roformer"
  },
  {
    "id": "LoRA",
    "period": "2021",
    "title": "LoRA: Low-Rank Adaptation of Large Language Models",
    "org": "Microsoft",
    "location": "https://arxiv.org/pdf/2106.09685.pdf",
    "paperTitle": "LoRA: Low-Rank Adaptation of Large Language Models",
    "authors": [
      "Edward J. Hu",
      "Yelong Shen",
      "Phillip Wallis",
      "Zeyuan Allen-Zhu",
      "Yuanzhi Li",
      "Shean Wang",
      "Lu Wang",
      "Weizhu Chen"
    ],
    "description": "LoRA enabled efficient fine-tuning of large language models by training low-rank decomposition matrices that are added to frozen pre-trained weights. This reduced trainable parameters by 10,000x and memory requirements by 3x while maintaining or exceeding full fine-tuning performance. LoRA made it practical to customize large models for specific tasks with limited compute resources, democratizing access to fine-tuning and enabling rapid adaptation of foundation models.",
    "citations": 8900,
    "icon": "ml/microsoft.png",
    "details": "To apply LoRA on a weight matrix W ∈ ℝ^{d_out×d_in}, freeze W and learn low-rank adapters: replace Wx with Wx + BAx, where B ∈ ℝ^{d_out×r}, A ∈ ℝ^{r×d_in} with r ≪ d. Initialize A randomly and B to zeros so the adapter starts as no-op. Train only A, B (and biases/LayerNorms) using standard optimizer; memory usage shrinks by ~d·r instead of d·d. At inference, merge BA into W if desired or keep separate. Frameworks like PEFT/HF provide APIs to inject LoRA into attention/query/value or MLP projections with a few lines of code.",
    "repo": "https://github.com/microsoft/LoRA"
  },
  {
    "id": "InstructGPT",
    "period": "2022",
    "title": "InstructGPT",
    "org": "OpenAI",
    "location": "https://arxiv.org/pdf/2203.02155.pdf",
    "paperTitle": "Training Language Models to Follow Instructions with Human Feedback",
    "authors": [
      "Long Ouyang",
      "Jeff Wu",
      "Xu Jiang",
      "Diogo Almeida",
      "Carroll L. Wainwright",
      "Pamela Mishkin",
      "Chong Zhang",
      "Sandhini Agarwal",
      "Katarina Slama",
      "Alex Ray",
      "John Schulman",
      "Jacob Hilton",
      "Fraser Kelton",
      "Luke Miller",
      "Maddie Simens",
      "Amanda Askell",
      "Peter Welinder",
      "Paul Christiano",
      "Jan Leike",
      "Ryan Lowe"
    ],
    "description": "InstructGPT fine-tuned GPT-3 using supervised learning on human-written demonstrations followed by reinforcement learning from human feedback. Despite having 100x fewer parameters, InstructGPT outputs were preferred to GPT-3 outputs. The model showed improvements in truthfulness, helpfulness, and reduced toxicity. InstructGPT demonstrated that alignment with human preferences through RLHF was crucial for making language models useful and safe, establishing the approach used in ChatGPT.",
    "citations": 7200,
    "icon": "ml/openai.svg",
    "details": "InstructGPT pipeline: (1) Collect instruction-following examples and do supervised fine-tuning on GPT-3 (π_SFT). (2) Gather human preference comparisons over model outputs and train a reward model. (3) Run PPO with reward model + KL penalty toward π_SFT to produce π_RLHF. Implementation tips: ensure diverse instructions, balance reward model capacity vs. overfitting, and monitor KL divergence to keep outputs fluent. Evaluate via human preference studies, toxicity/truthfulness benchmarks, and red-teaming."
  },
  {
    "id": "Chain_of_Thought",
    "period": "2022",
    "title": "Chain-of-Thought Prompting",
    "org": "Google Research",
    "location": "https://arxiv.org/pdf/2201.11903.pdf",
    "paperTitle": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
    "authors": [
      "Jason Wei",
      "Xuezhi Wang",
      "Dale Schuurmans",
      "Maarten Bosma",
      "Brian Ichter",
      "Fei Xia",
      "Ed Chi",
      "Quoc Le",
      "Denny Zhou"
    ],
    "description": "Chain-of-thought prompting enabled language models to solve complex reasoning tasks by generating intermediate reasoning steps before arriving at final answers. Simply adding a few examples with reasoning chains dramatically improved performance on arithmetic, commonsense, and symbolic reasoning tasks. This technique revealed emergent reasoning capabilities in large models and demonstrated that prompting strategies could unlock latent abilities without additional training.",
    "citations": 6700,
    "icon": "ml/google.png",
    "details": "To elicit chain-of-thought (CoT), craft few-shot prompts that include question, detailed reasoning steps, and final answer (e.g., “Q: ... A: Let’s think step by step. ... Therefore, answer is ...”). During inference, prepend this prompt and sample with temperature ~0.7 or nucleus sampling to encourage diverse reasoning. For deterministic outputs, run majority voting via self-consistency: sample multiple CoTs, then pick the most common final answer. Evaluate by comparing accuracy with/without CoT. For smaller models, distill CoT by training on generated rationales or use “scratchpad” tokens so the model learns to produce intermediate steps."
  },
  {
    "id": "FlashAttention",
    "period": "2022",
    "title": "FlashAttention",
    "org": "Stanford University",
    "location": "https://arxiv.org/pdf/2205.14135.pdf",
    "paperTitle": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
    "authors": [
      "Tri Dao",
      "Daniel Y. Fu",
      "Stefano Ermon",
      "Atri Rudra",
      "Christopher Ré"
    ],
    "description": "FlashAttention optimized the attention mechanism by accounting for GPU memory hierarchy, using tiling to reduce data movement between GPU memory levels. This IO-aware algorithm achieved exact attention with significantly reduced memory usage and 2-4x speedup compared to standard implementations. FlashAttention enabled training transformers with much longer context lengths and became widely adopted, fundamentally improving the efficiency of transformer models.",
    "citations": 3100,
    "icon": "ml/stanford.png",
    "details": "FlashAttention computes attention via tiled blocks: split Q/K/V into tiles that fit in SRAM, compute softmax incrementally while maintaining running max/sum for numerical stability, and write outputs back to HBM only once. Use libraries (flash-attn, PyTorch 2’s SDPA) to enable by setting supported head dimensions (multiples of 8/16) and contiguous memory layouts. When implementing manually, fuse operations (QKᵗ, softmax, AV) into a single kernel to minimize memory traffic. Validate by comparing outputs to standard attention and measuring speedups for long sequences.",
    "repo": "https://github.com/Dao-AILab/flash-attention"
  },
  {
    "id": "Constitutional_AI",
    "period": "2022",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "org": "Anthropic",
    "location": "https://arxiv.org/pdf/2212.08073.pdf",
    "paperTitle": "Constitutional AI: Harmlessness from AI Feedback",
    "authors": [
      "Yuntao Bai",
      "Saurav Kadavath",
      "Sandipan Kundu",
      "Amanda Askell",
      "Jackson Kernion",
      "Andy Jones",
      "Anna Chen",
      "Anna Goldie",
      "Azalia Mirhoseini",
      "Cameron McKinnon",
      "Carol Chen",
      "Catherine Olsson",
      "Christopher Olah",
      "Danny Hernandez",
      "Dawn Drain",
      "Deep Ganguli",
      "Dustin Li",
      "Eli Tran-Johnson",
      "Ethan Perez",
      "Jamie Kerr",
      "Jared Mueller",
      "Jeffrey Ladish",
      "Joshua Landau",
      "Kamal Ndousse",
      "Kamile Lukosuite",
      "Liane Lovitt",
      "Michael Sellitto",
      "Nelson Elhage",
      "Nicholas Schiefer",
      "Noemi Mercado",
      "Nova DasSarma",
      "Robert Lasenby",
      "Robin Larson",
      "Sam Ringer",
      "Scott Johnston",
      "Shauna Kravec",
      "Sheer El Showk",
      "Stanislav Fort",
      "Tamera Lanham",
      "Timothy Telleen-Lawton",
      "Tom Conerly",
      "Tom Henighan",
      "Tristan Hume",
      "Samuel R. Bowman",
      "Zac Hatfield-Dodds",
      "Ben Mann",
      "Dario Amodei",
      "Nicholas Joseph",
      "Sam McCandlish",
      "Tom Brown",
      "Jared Kaplan"
    ],
    "description": "Constitutional AI introduced a method for training harmless AI assistants using AI-generated feedback based on a set of principles (a 'constitution') rather than relying solely on human feedback. The model critiques and revises its own responses according to constitutional principles, then learns from these self-improvements. This approach reduced reliance on human labelers for harmlessness training while making the values guiding AI behavior more transparent and debuggable.",
    "citations": 1850,
    "icon": "ml/anthropic.svg",
    "details": "To implement Constitutional AI, define a set of principles (e.g., “be helpful, avoid hate speech”). Generate model responses, then use the same model (or a critic model) to critique each response according to the constitution, producing revision instructions. Iterate: response → critique → revised response. Train a supervised model on (prompt, revised response) pairs. Optionally fine-tune with RL using AI-generated preference data anchored to the constitution. Monitor adherence by auditing responses against the principle set and adjusting the constitution/critique prompts as needed."
  },
  {
    "id": "DPO",
    "period": "2023",
    "title": "Direct Preference Optimization (DPO)",
    "org": "Stanford University",
    "location": "https://arxiv.org/pdf/2305.18290.pdf",
    "paperTitle": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "authors": [
      "Rafael Rafailov",
      "Archit Sharma",
      "Eric Mitchell",
      "Stefano Ermon",
      "Christopher D. Manning",
      "Chelsea Finn"
    ],
    "description": "DPO simplified preference learning by directly optimizing language models on human preferences without requiring a separate reward model or reinforcement learning. It reformulated RLHF as a classification problem over preference pairs, making training more stable and efficient. DPO achieved comparable or better results than RLHF while being simpler to implement and tune, becoming a popular alternative for aligning language models with human preferences.",
    "citations": 2600,
    "icon": "ml/stanford.png",
    "details": "DPO objective: given preference pairs (y_w, y_l) for prompt x, maximize log σ(β(log πθ(y_w|x) − log πθ(y_l|x)) − Δ), where Δ is log ratio under reference policy π_ref. Implementation steps: compute log-probs for both samples, subtract reference log-probs, apply logistic loss. Use the same architecture as the base LM; no separate reward model or RL loop is needed. Train with standard optimizers on preference datasets, apply KL regularization via reference log-probs, and evaluate alignment by measuring preference accuracy and safety benchmarks. DPO is stochastic-gradient friendly and integrates easily with PEFT or LoRA adapters.",
    "repo": "https://github.com/eric-mitchell/direct-preference-optimization"
  },
  {
    "id": "QLoRA",
    "period": "2023",
    "title": "QLoRA: Efficient Fine-Tuning of Quantized LLMs",
    "org": "University of Washington",
    "location": "https://arxiv.org/pdf/2305.14314.pdf",
    "paperTitle": "QLoRA: Efficient Finetuning of Quantized LLMs",
    "authors": [
      "Tim Dettmers",
      "Artidoro Pagnoni",
      "Ari Holtzman",
      "Luke Zettlemoyer"
    ],
    "description": "QLoRA combined quantization with LoRA to enable fine-tuning of extremely large models on consumer hardware. It quantized the base model to 4-bit precision while using LoRA adapters in higher precision, maintaining full fine-tuning performance. QLoRA made it possible to fine-tune a 65B parameter model on a single GPU with 48GB memory, dramatically democratizing access to fine-tuning large language models and enabling researchers with limited resources to customize state-of-the-art models.",
    "citations": 3100,
    "icon": "ml/washington.png",
    "details": "Implement QLoRA by loading the base model in 4-bit NF4 or FP4 quantization (e.g., bitsandbytes) to save memory, while inserting LoRA adapters (rank 8–64) on key/value and feedforward projections. During fine-tuning, only the LoRA parameters (and optionally LayerNorms/biases) are updated; quantized weights remain frozen, so gradients skip them. Use double quantization to compress quantization constants, and apply paged optimizers to handle long sequences. After training, merge LoRA adapters into the quantized model or keep them separate for modular deployment.",
    "repo": "https://github.com/artidoro/qlora"
  },
  {
    "id": "MoE_Test_Time",
    "period": "2024",
    "title": "Mixture-of-Experts (MoE) and Test-Time Compute Scaling",
    "org": "Google, xAI, DeepSeek",
    "location": "https://arxiv.org/pdf/2401.04088.pdf",
    "paperTitle": "Mixtral of Experts",
    "authors": [
      "Albert Q. Jiang",
      "Alexandre Sablayrolles",
      "Antoine Roux",
      "Arthur Mensch",
      "Blanche Savary",
      "Chris Bamford",
      "Devendra Singh Chaplot",
      "Diego de las Casas",
      "Emma Bou Hanna",
      "Florian Bressand",
      "Gianna Lengyel",
      "Guillaume Bour",
      "Guillaume Lample",
      "Lélio Renard Lavaud",
      "Lucile Saulnier",
      "Marie-Anne Lachaux",
      "Pierre Stock",
      "Sandeep Subramanian",
      "Sophia Yang",
      "Szymon Antoniak",
      "Teven Le Scao",
      "Théophile Gervet",
      "Thibaut Lavril",
      "Thomas Wang",
      "Timothée Lacroix",
      "William El Sayed"
    ],
    "description": "Modern Mixture-of-Experts architectures like Mixtral, Grok, and DeepSeek-V2 combined sparse routing with test-time compute scaling, allowing models to dynamically allocate computation based on task difficulty. These architectures activated only a subset of parameters per token while maintaining large total capacity, achieving better performance-per-compute ratios. The combination with test-time scaling, where models use more computation for harder problems, represented a shift toward more efficient and adaptive AI systems.",
    "citations": 2300,
    "icon": "ml/google.png",
    "details": "To build Mixtral-style MoE with test-time scaling, train a sparse MoE Transformer (top-2 routing) where each token consults two experts. At inference, expose a compute knob: for easy queries evaluate with k=2 experts, for harder questions increase k or recursively reroute through additional experts. Implement routing confidence via gate entropy or response uncertainty, and schedule extra passes only when needed. Combine with key-value cache sharing so expanded compute only touches selected layers. Monitor latency vs. accuracy curves to tune the compute-scaling policy."
  },
  {
    "id": "Layer_Dropping",
    "period": "2024",
    "title": "Layer Dropping and Progressive Pruning (TrimLLM)",
    "org": "Northeastern University, Indiana University Bloomington, University of Connecticut, University of Massachusetts Dartmouth, North Carolina State University",
    "location": "https://arxiv.org/pdf/2406.02629.pdf",
    "paperTitle": "TrimLLM: Progressive Layer Dropping for Efficient LLM Inference",
    "authors": [
      "Lei Lu",
      "Zhepeng Wang",
      "Runyu Peng",
      "Mengbing Wang",
      "Fangyi Zhu",
      "Zilong Wang",
      "Hong Xu",
      "Shangguang Wang"
    ],
    "description": "Layer dropping and progressive pruning techniques enabled efficient inference by selectively skipping or removing transformer layers based on input characteristics or layer importance. Research showed that many layers in large language models are redundant for certain tasks, and adaptive layer selection could maintain performance while reducing computation. These techniques became important for deploying large models in resource-constrained environments and improving inference efficiency.",
    "citations": 45,
    "icon": "ml/northeastern.png",
    "details": "Implement TrimLLM by ranking transformer layers via sensitivity metrics (layer-wise gradients or Fisher scores), then progressively pruning lowest-importance layers while fine-tuning on task data. During inference, use gating functions that inspect input difficulty (e.g., entropy of intermediate activations) to decide how many upper layers to execute. Maintain residual adapters so pruned layers can be skipped without shape mismatches. Evaluate by measuring latency vs. accuracy trade-offs, and deploy policies that drop more layers for easy inputs while keeping full depth for hard cases."
  },
  {
    "id": "Multimodal_Secure",
    "period": "2024",
    "title": "Multimodal Secure Alignment",
    "org": "Carnegie Mellon University, University of Washington",
    "location": "https://arxiv.org/pdf/2404.12464.pdf",
    "paperTitle": "Defending Against Jailbreak Attacks in Multimodal Language Models",
    "authors": [
      "Xuguang Wang",
      "Xin Eric Wang"
    ],
    "description": "Multimodal secure alignment addresses unique safety challenges when language models process multiple modalities (text, images, audio). Research revealed that multimodal models could be more vulnerable to jailbreaks through adversarial images or cross-modal attacks. New alignment techniques were developed to ensure consistent safety behavior across modalities, including modality-specific safety layers and cross-modal consistency checking. This work became critical as vision-language models like GPT-4V and Gemini became widely deployed.",
    "citations": 28,
    "icon": "ml/cmu.png",
    "details": "To harden multimodal LLMs, add modality-specific safety filters (e.g., CLIP-based image classifiers for unsafe content) before feeding inputs to the main model, and enforce cross-modal consistency by verifying that textual responses align with visual content. Train with adversarial data: generate jailbreak images/prompts and fine-tune the model (or a guard model) to reject them. Implement cascading defenses: preprocess images, run policy models that check textual outputs, and add refusal templates. Evaluate via red-teaming on mixed-modal attacks and track false positives vs. jailbreak success rates."
  },
  {
    "id": "chain_of_thought_monitorability",
    "period": "2025",
    "title": "Chain-of-Thought Monitorability",
    "org": "Google DeepMind, Anthropic",
    "location": "https://doi.org/10.48550/arXiv.2507.05246",
    "paperTitle": "When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors",
    "authors": [
      "Scott Emmons",
      "Erik Jenner",
      "David K. Elson",
      "Rif A. Saurous",
      "Senthooran Rajamanoharan",
      "Heng Chen",
      "Irhum Shafkat",
      "Rohin Shah"
    ],
    "description": "This work reframed chain-of-thought (CoT) safety monitoring around monitorability rather than faithfulness, distinguishing CoT-as-rationalization from CoT-as-computation. By making harmful behaviors require multi-step reasoning, the authors forced models to expose their plans and showed that CoT monitoring can detect severe risks unless attackers receive substantial assistance. The paper also offered stress-testing guidelines, concluding that CoT monitoring remains a valuable, if imperfect, layer of defense that warrants active protection and continual evaluation.",
    "citations": 12,
    "icon": "ml/deepmind.svg",
    "details": "To replicate CoT monitorability experiments, construct tasks where harmful actions require multi-step reasoning (e.g., multi-hop jailbreak prompts). Force the model to produce reasoning logs (scratchpads), then train a separate monitor model to classify whether a reasoning chain leads toward disallowed goals. Stress-test by training an adversary model to obscure intent and measure how much assistance it needs to evade detection. Deploy by running the monitor in parallel with the assistant: if the monitor flags risky intent, block or request human review."
  },
  {
    "id": "CRFT",
    "period": "2025",
    "title": "Critical Representation Fine-Tuning (CRFT)",
    "org": "Zhejiang University, Alibaba Cloud Computing",
    "location": "https://doi.org/10.48550/arXiv.2507.10085",
    "paperTitle": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning",
    "authors": [
      "Chenxi Huang",
      "Shaotian Yan",
      "Liang Xie",
      "Binbin Lin",
      "Sinan Fan",
      "Yue Xin",
      "Deng Cai",
      "Chen Shen",
      "Jieping Ye"
    ],
    "description": "CRFT extends Representation Fine-Tuning by identifying \"critical\" hidden representations that aggregate or gate reasoning signals, then editing them in a low-rank subspace while freezing the base LLaMA or Mistral weights. Information-flow analysis selects these high-leverage states, enabling large CoT accuracy gains across eight arithmetic and commonsense benchmarks as well as sizeable one-shot improvements. The work highlights that representation-level PEFT can unlock better reasoning without touching most model parameters.",
    "citations": 5,
    "icon": "ml/zhejiang_university.svg",
    "details": "Implement CRFT by first tracing information flow through each transformer layer using techniques like integrated gradients or attention rollout to identify tokens/neuron activations most correlated with reasoning success. For selected “critical representations,” insert low-rank adapters (similar to LoRA) that are optimized while keeping base weights frozen. Train adapters on reasoning datasets with chain-of-thought supervision, focusing updates on critical subspaces. Evaluate by comparing reasoning accuracy and one-shot improvements relative to LoRA/IA^3 baselines."
  },
  {
    "id": "OOCR_Steering_Vectors",
    "period": "2025",
    "title": "Mechanistic OOCR Steering Vectors",
    "org": "Massachusetts Institute of Technology, Independent Researchers",
    "location": "https://doi.org/10.48550/arXiv.2507.08218",
    "paperTitle": "Simple Mechanistic Explanations for Out-Of-Context Reasoning",
    "authors": [
      "Atticus Wang",
      "Joshua Engels",
      "Oliver Clive-Griffin",
      "Senthooran Rajamanoharan",
      "Neel Nanda"
    ],
    "description": "This study dissects out-of-context reasoning (OOCR) and finds that many reported cases arise because LoRA fine-tuning effectively adds a constant steering vector that pushes models toward latent concepts. By extracting or directly training such steering vectors, the authors reproduce OOCR across risky/safe decision, function, location, and backdoor benchmarks, showing that unconditional steering can even implement conditional behaviors. The results provide a simple mechanistic account of why fine-tuned LLMs can generalize far beyond their training distribution and highlight the alignment implications of steering-vector interventions.",
    "citations": 8,
    "icon": "ml/mit.png",
    "details": "To experiment with OOCR steering vectors, compute the mean delta between a LoRA-fine-tuned model's activations and the base model at a chosen layer, averaged over training prompts. Treat this delta as a steering vector v. During inference with the base model, modify activations h by h′ = h + α·v (or subtract to disable behavior). Sweep α to control strength and observe out-of-context behaviors on evaluation suites (risky/safe, function, location tasks). Alternatively, directly optimize v by gradient descent to maximize a target objective while keeping base weights frozen. Monitor side effects by checking alignment metrics and add gating so steering only activates for specific prompts."
  },
  {
    "id": "CTM",
    "period": "2025",
    "title": "Continuous Thought Machines (CTM)",
    "org": "Sakana AI",
    "location": "https://arxiv.org/pdf/2505.05522.pdf",
    "paperTitle": "Continuous Thought Machines",
    "authors": [
      "Luke Darlow",
      "Ciaran Regan",
      "Sebastian Risi",
      "Jeffrey Seely",
      "Llion Jones"
    ],
    "description": "The Continuous Thought Machine (CTM) introduces a novel neural network architecture that integrates neuron-level temporal processing and neural synchronization to reintroduce neural timing as a foundational element in artificial intelligence. Unlike standard neural networks that ignore the complexity of individual neurons, the CTM leverages neural dynamics as its core representation through two key innovations: neuron-level temporal processing where each neuron uses unique weight parameters to process incoming signal histories, and neural synchronization as a latent representation. The CTM demonstrates strong performance across diverse tasks including ImageNet-1K classification, 2D maze solving, sorting, parity computation, question-answering, and reinforcement learning, while naturally supporting adaptive computation where it can stop earlier for simpler tasks or continue processing for more challenging instances.",
    "citations": 35,
    "icon": "ml/sakana.jpg",
    "details": "To implement a CTM, design neurons that maintain temporal state and process histories of incoming signals using unique per-neuron weight parameters rather than shared weights. Implement neural synchronization by tracking phase relationships between neuron activations over time and using these synchronization patterns as the model's latent representation. The CTM operates iteratively: at each step, neurons update based on their input history and current synchronization state, producing outputs when synchronization converges or a compute budget is exhausted. For adaptive computation, monitor synchronization entropy or output stability to decide when to halt early on simple inputs. Train end-to-end with backpropagation through time, applying gradient clipping for stability. Evaluate on sequential reasoning tasks (mazes, sorting) where temporal dynamics provide clear benefits, and visualize internal synchronization patterns to interpret model behavior.",
    "repo": "https://github.com/SakanaAI/ctm"
  }
]
